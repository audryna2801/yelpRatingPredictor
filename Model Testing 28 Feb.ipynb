{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import linear_model, tree, neighbors\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from nltk import pos_tag\n",
    "import time\n",
    "from model import*\n",
    "from analyze_words import get_df_idf_stops\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze_words import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyModels(model, x_train, y_train):\n",
    "    print(model)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predictModel(model, x_test):\n",
    "    prediction = model.predict(x_test)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.6 s, sys: 2.79 s, total: 37.4 s\n",
      "Wall time: 38.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df, idf, stop_words = get_df_idf_stops('test_data/merged_data.csv', n=1, lemmatized=False,num_stop_words=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformFeatureSelection(model, x):\n",
    "    return model.transform(x)\n",
    "\n",
    "def applyFeatureSelection(model, x_train, y_train):\n",
    "    model = model.fit(x_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.95      0.97      1973\n",
      "           2       0.96      0.99      0.98      1855\n",
      "           3       0.88      0.94      0.91      1786\n",
      "           4       0.78      0.74      0.76      1981\n",
      "           5       0.77      0.77      0.77      1905\n",
      "\n",
      "    accuracy                           0.88      9500\n",
      "   macro avg       0.88      0.88      0.88      9500\n",
      "weighted avg       0.88      0.88      0.87      9500\n",
      "\n",
      "Accuracy Score\n",
      "0.9601052631578948\n"
     ]
    }
   ],
   "source": [
    "\n",
    "comb = {\"ngram\": 1, \"lemmatize\": False, \"stop_word\": 0}\n",
    "\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(df.drop(\"Rating\", axis=1),\n",
    "                                         df.Rating,\n",
    "                                         test_size=0.95,\n",
    "                                         random_state=33)\n",
    "\n",
    "# Feature Selection\n",
    "model = linear_model.SGDClassifier(alpha=0.1)\n",
    "trained_model = applyModels(model, x_train, y_train)\n",
    "feature_selection_model = SelectFromModel(trained_model)\n",
    "trained_feature_selection_model = applyFeatureSelection(feature_selection_model,\n",
    "                                                        x_train, y_train)\n",
    "x_train = transformFeatureSelection(trained_feature_selection_model,\n",
    "                                    x_train)\n",
    "x_test = transformFeatureSelection(trained_feature_selection_model,\n",
    "                                   x_test)\n",
    "\n",
    "final_model = applyModels(model, x_train, y_train)\n",
    "prediction = predictModel(final_model, x_test)\n",
    "\n",
    "print(\"Final Model Classification Report\")\n",
    "print(classification_report(prediction, y_test))\n",
    "print(\"Accuracy Score\")\n",
    "print(evaluateModel(prediction, y_test))\n",
    "\n",
    "# Save best Model\n",
    "joblib.dump(final_model, \"optimal_args/final_model.pkl\")\n",
    "\n",
    "# Save best columns, idf, combination, and stop words\n",
    "feature_idx = trained_feature_selection_model.get_support()\n",
    "column_names = df.drop(\"Rating\", axis=1).columns[feature_idx]\n",
    "with open('optimal_args/columns.json', 'w') as f:\n",
    "    json.dump(list(column_names), f)\n",
    "with open('optimal_args/idf.json', 'w') as f:\n",
    "    json.dump(idf, f)\n",
    "with open('optimal_args/combination.json', 'w') as f:\n",
    "    json.dump(comb, f)\n",
    "with open('optimal_args/num_stop_words.json', 'w') as f:\n",
    "    json.dump(stop_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(\"Rating\", axis=1),df.Rating,test_size=0.9)\n",
    "model = linear_model.SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier()\n"
     ]
    }
   ],
   "source": [
    "trained_model = applyModels(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_model = SelectFromModel(trained_model)\n",
    "trained_feature_selection_model = applyFeatureSelection(feature_selection_model,\n",
    "                                                        x_train, y_train)\n",
    "x_train = transformFeatureSelection(trained_feature_selection_model,\n",
    "                                    x_train)\n",
    "x_test = transformFeatureSelection(trained_feature_selection_model,\n",
    "                                   x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 1.36605709, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = trained_feature_selection_model.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = df.drop(\"Rating\", axis=1).columns[feature_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier()\n"
     ]
    }
   ],
   "source": [
    "final_model = applyModels(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictModel(final_model, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9264444444444444"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prediction == y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861041666666667"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_df = pd.DataFrame({'predict': prediction,\n",
    "                             'actual': y_test}).astype('int')\n",
    "pred_test_df['difference'] = (pred_test_df.predict -\n",
    "                              pred_test_df.actual).abs()\n",
    "pred_test_df['weighted_difference'] = pred_test_df['difference'] * \\\n",
    "    pred_test_df['difference']\n",
    "\n",
    "num_tests = len(pred_test_df.index)\n",
    "total_deviance = pred_test_df['weighted_difference'].sum()\n",
    "\n",
    "# maximum deviance is 4 (5 star rating vs 1 star rating)\n",
    "weighted_accuracy = 1 - (total_deviance / (4 * 4 * num_tests))\n",
    "weighted_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9724166666666667"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def evaluateModel(prediction, y_test):\n",
    "    '''\n",
    "    Calculate the accuracy of model based on the proportion of accurate \n",
    "    predictions using the testing data. Calculation is weighted by the degree of\n",
    "    deviance from 'actual rating'.\n",
    "    '''\n",
    "    # convert into pandas dataframe for easier handling\n",
    "    pred_test_df = pd.DataFrame({'predict': prediction,\n",
    "                                 'actual': y_test}).astype('int')\n",
    "    pred_test_df['difference'] = (pred_test_df.predict -\n",
    "                                  pred_test_df.actual).abs()\n",
    "\n",
    "    num_tests = len(pred_test_df.index)\n",
    "    total_deviance = pred_test_df['difference'].sum()\n",
    "\n",
    "    # maximum deviance is 4 (5 star rating vs 1 star rating)\n",
    "    weighted_accuracy = 1 - (total_deviance / (4 * num_tests))\n",
    "\n",
    "    return weighted_accuracy\n",
    "\n",
    "evaluateModel(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, True, 0, 0.0001),\n",
       " (1, True, 0, 0.001),\n",
       " (1, True, 0, 0.01),\n",
       " (1, True, 0, 0.1),\n",
       " (1, True, 0, 1),\n",
       " (1, True, 10, 0.0001),\n",
       " (1, True, 10, 0.001),\n",
       " (1, True, 10, 0.01),\n",
       " (1, True, 10, 0.1),\n",
       " (1, True, 10, 1),\n",
       " (1, True, 20, 0.0001),\n",
       " (1, True, 20, 0.001),\n",
       " (1, True, 20, 0.01),\n",
       " (1, True, 20, 0.1),\n",
       " (1, True, 20, 1),\n",
       " (1, False, 0, 0.0001),\n",
       " (1, False, 0, 0.001),\n",
       " (1, False, 0, 0.01),\n",
       " (1, False, 0, 0.1),\n",
       " (1, False, 0, 1),\n",
       " (1, False, 10, 0.0001),\n",
       " (1, False, 10, 0.001),\n",
       " (1, False, 10, 0.01),\n",
       " (1, False, 10, 0.1),\n",
       " (1, False, 10, 1),\n",
       " (1, False, 20, 0.0001),\n",
       " (1, False, 20, 0.001),\n",
       " (1, False, 20, 0.01),\n",
       " (1, False, 20, 0.1),\n",
       " (1, False, 20, 1),\n",
       " (2, True, 0, 0.0001),\n",
       " (2, True, 0, 0.001),\n",
       " (2, True, 0, 0.01),\n",
       " (2, True, 0, 0.1),\n",
       " (2, True, 0, 1),\n",
       " (2, True, 10, 0.0001),\n",
       " (2, True, 10, 0.001),\n",
       " (2, True, 10, 0.01),\n",
       " (2, True, 10, 0.1),\n",
       " (2, True, 10, 1),\n",
       " (2, True, 20, 0.0001),\n",
       " (2, True, 20, 0.001),\n",
       " (2, True, 20, 0.01),\n",
       " (2, True, 20, 0.1),\n",
       " (2, True, 20, 1),\n",
       " (2, False, 0, 0.0001),\n",
       " (2, False, 0, 0.001),\n",
       " (2, False, 0, 0.01),\n",
       " (2, False, 0, 0.1),\n",
       " (2, False, 0, 1),\n",
       " (2, False, 10, 0.0001),\n",
       " (2, False, 10, 0.001),\n",
       " (2, False, 10, 0.01),\n",
       " (2, False, 10, 0.1),\n",
       " (2, False, 10, 1),\n",
       " (2, False, 20, 0.0001),\n",
       " (2, False, 20, 0.001),\n",
       " (2, False, 20, 0.01),\n",
       " (2, False, 20, 0.1),\n",
       " (2, False, 20, 1),\n",
       " (3, True, 0, 0.0001),\n",
       " (3, True, 0, 0.001),\n",
       " (3, True, 0, 0.01),\n",
       " (3, True, 0, 0.1),\n",
       " (3, True, 0, 1),\n",
       " (3, True, 10, 0.0001),\n",
       " (3, True, 10, 0.001),\n",
       " (3, True, 10, 0.01),\n",
       " (3, True, 10, 0.1),\n",
       " (3, True, 10, 1),\n",
       " (3, True, 20, 0.0001),\n",
       " (3, True, 20, 0.001),\n",
       " (3, True, 20, 0.01),\n",
       " (3, True, 20, 0.1),\n",
       " (3, True, 20, 1),\n",
       " (3, False, 0, 0.0001),\n",
       " (3, False, 0, 0.001),\n",
       " (3, False, 0, 0.01),\n",
       " (3, False, 0, 0.1),\n",
       " (3, False, 0, 1),\n",
       " (3, False, 10, 0.0001),\n",
       " (3, False, 10, 0.001),\n",
       " (3, False, 10, 0.01),\n",
       " (3, False, 10, 0.1),\n",
       " (3, False, 10, 1),\n",
       " (3, False, 20, 0.0001),\n",
       " (3, False, 20, 0.001),\n",
       " (3, False, 20, 0.01),\n",
       " (3, False, 20, 0.1),\n",
       " (3, False, 20, 1),\n",
       " (4, True, 0, 0.0001),\n",
       " (4, True, 0, 0.001),\n",
       " (4, True, 0, 0.01),\n",
       " (4, True, 0, 0.1),\n",
       " (4, True, 0, 1),\n",
       " (4, True, 10, 0.0001),\n",
       " (4, True, 10, 0.001),\n",
       " (4, True, 10, 0.01),\n",
       " (4, True, 10, 0.1),\n",
       " (4, True, 10, 1),\n",
       " (4, True, 20, 0.0001),\n",
       " (4, True, 20, 0.001),\n",
       " (4, True, 20, 0.01),\n",
       " (4, True, 20, 0.1),\n",
       " (4, True, 20, 1),\n",
       " (4, False, 0, 0.0001),\n",
       " (4, False, 0, 0.001),\n",
       " (4, False, 0, 0.01),\n",
       " (4, False, 0, 0.1),\n",
       " (4, False, 0, 1),\n",
       " (4, False, 10, 0.0001),\n",
       " (4, False, 10, 0.001),\n",
       " (4, False, 10, 0.01),\n",
       " (4, False, 10, 0.1),\n",
       " (4, False, 10, 1),\n",
       " (4, False, 20, 0.0001),\n",
       " (4, False, 20, 0.001),\n",
       " (4, False, 20, 0.01),\n",
       " (4, False, 20, 0.1),\n",
       " (4, False, 20, 1),\n",
       " (5, True, 0, 0.0001),\n",
       " (5, True, 0, 0.001),\n",
       " (5, True, 0, 0.01),\n",
       " (5, True, 0, 0.1),\n",
       " (5, True, 0, 1),\n",
       " (5, True, 10, 0.0001),\n",
       " (5, True, 10, 0.001),\n",
       " (5, True, 10, 0.01),\n",
       " (5, True, 10, 0.1),\n",
       " (5, True, 10, 1),\n",
       " (5, True, 20, 0.0001),\n",
       " (5, True, 20, 0.001),\n",
       " (5, True, 20, 0.01),\n",
       " (5, True, 20, 0.1),\n",
       " (5, True, 20, 1),\n",
       " (5, False, 0, 0.0001),\n",
       " (5, False, 0, 0.001),\n",
       " (5, False, 0, 0.01),\n",
       " (5, False, 0, 0.1),\n",
       " (5, False, 0, 1),\n",
       " (5, False, 10, 0.0001),\n",
       " (5, False, 10, 0.001),\n",
       " (5, False, 10, 0.01),\n",
       " (5, False, 10, 0.1),\n",
       " (5, False, 10, 1),\n",
       " (5, False, 20, 0.0001),\n",
       " (5, False, 20, 0.001),\n",
       " (5, False, 20, 0.01),\n",
       " (5, False, 20, 0.1),\n",
       " (5, False, 20, 1)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams = [1, 2, 3, 4, 5]\n",
    "lemmatizes = [True, False]\n",
    "stop_words = [0, 10, 20]\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "\n",
    "import itertools\n",
    "list(itertools.product(ngrams,lemmatizes, stop_words,alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = {\"ngram\": 2, \"lemmatize\": True, \"stop_word\": 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('columns.json', 'w') as f:\n",
    "        json.dump(list(feature_name), f)\n",
    "with open('idf.json', 'w') as f:\n",
    "    json.dump(idf, f)\n",
    "with open('combination.json', 'w') as f:\n",
    "    json.dump(comb, f)\n",
    "with open('stop_words.json', 'w') as f:\n",
    "    json.dump(stop_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import process_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.8 ms, sys: 4.56 ms, total: 51.3 ms\n",
      "Wall time: 51.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_array = process_input(\"\"\"Due to COVID we decided to enjoy an at home Valentine's Day dinner via a meal kit. Sadly it was a mistake. It was not a meal kit for 4 as described online and in their instructions, but rather 2.\n",
    "\n",
    "Total Score: 2.4/5\n",
    "Ease of Pickup: 2/5 Naperville-They sat in their truck playing on their phones waiting to set up and made us wait 20 minutes. Finally someone demanded theirs so we got ours too.\n",
    "Quality of ingredients: 5/5\n",
    "Quality of prepared food: 3/5 - it was akin to take and bake pizza.\n",
    "Accuracy of kit: 1/5 - Only received pig face for two with a total weight of 9 ounces and  two chocolate chip cookies. The empanadas and green beans were as described.\n",
    "Value: 1/5\n",
    "\n",
    "I contacted the restaurant. They told me the ingredients I received were correct except I should have received 4 cookies. They also said it had changed. When? Before or after The Girl and The Goat printed the instructions, advertised the meal kit, or I paid $133 for a 4 person dinner. To their credit they did refund my money. Such a disappointing experience from one of my former favorite restaurants\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['got', 'so', 'or', 'after', 'two', 'have', 'received', 'refund',\n",
       "       'waiting', 'an',\n",
       "       ...\n",
       "       'phones', 'their phones', 'covid decided', 'credit', 'face', 'accuracy',\n",
       "       'so got', 'theirs', 'prepared food', 'score'],\n",
       "      dtype='object', length=106)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_name[x_array.nonzero()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26360"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 26360)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictModel(final_model, [x_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(df):\n",
    "    '''\n",
    "    Given a dataset with P predictor variables, uses forward selection to\n",
    "    select models for every value of K between 1 and P.\n",
    "\n",
    "    Inputs:\n",
    "        dataset: (DataSet object) a dataset\n",
    "\n",
    "    Returns:\n",
    "        A list (of length P) of Model objects. The first element is the\n",
    "        model where K=1, the second element is the model where K=2, and so on.\n",
    "    '''\n",
    "\n",
    "    models = []\n",
    "    pred_vars_avail = dataset.pred_vars[:]\n",
    "    pred_vars_used = []\n",
    "    num_vars = len(dataset.pred_vars)\n",
    "\n",
    "    for _ in range(num_vars):\n",
    "        max_R2 = 0\n",
    "        max_model = None\n",
    "        max_variable = None\n",
    "        for var in pred_vars_avail:\n",
    "            model = Model(dataset, pred_vars_used + [var])\n",
    "            if model.R2 > max_R2:\n",
    "                max_model = model\n",
    "                max_R2 = model.R2\n",
    "                max_variable = var\n",
    "        pred_vars_used.append(max_variable)\n",
    "        pred_vars_avail.remove(max_variable)\n",
    "        models.append(max_model)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
