{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jameshu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# analyze.py\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import itertools\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Generating global variables\n",
    "STOP_PREFIXES = (\"@\", \"#\", \"http\", \"&amp\")\n",
    "\n",
    "\n",
    "def keep_chr(ch):\n",
    "    '''\n",
    "    Find all characters that are classifed as punctuation\n",
    "    in Unicode and combine them into a single string.\n",
    "    \n",
    "    Inputs:\n",
    "      - ch (str): Unicode character\n",
    "        \n",
    "    Returns: Boolean\n",
    "    '''\n",
    "    return unicodedata.category(ch).startswith('P')\n",
    "\n",
    "\n",
    "PUNCTUATION = \" \".join([chr(i) for i in range(sys.maxunicode)\n",
    "                        if keep_chr(chr(i))])\n",
    "\n",
    "\n",
    "# Pre-processing stage\n",
    "def processing(text, lemmatized):\n",
    "    '''\n",
    "    Convert a text of a review into a list of strings.\n",
    "\n",
    "    Inputs:\n",
    "      - text (str): text representing one review\n",
    "\n",
    "    Returns: list of words\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    split_text = text.split()\n",
    "    new_text = []\n",
    "\n",
    "    for word in split_text:\n",
    "        # Handle trailing and internal punctuation\n",
    "        word = word.strip(PUNCTUATION)\n",
    "        word = word.replace(\"&apos;\", \"'\")\n",
    "        word = word.replace(\"quot;\", '\"')\n",
    "        word = word.replace(\"&quot\", '\"')\n",
    "\n",
    "        word = word.lower()\n",
    "\n",
    "        if lemmatized:\n",
    "            lemmatizer.lemmatize(word)\n",
    "\n",
    "        # Split word if \"/\" present\n",
    "        if \"/\" in word:\n",
    "            words = word.split(\"/\")\n",
    "            new_text += [word for word in words if not\n",
    "                         bool(re.search(r\"\\d\", word)) and\n",
    "                         not word.startswith(STOP_PREFIXES)]\n",
    "        elif (word and not bool(re.search(r\"\\d\", word))\n",
    "              and not word.startswith(STOP_PREFIXES)):\n",
    "            new_text.append(word)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def get_stop_words(all_tokens, num_stop_words=20):\n",
    "    '''\n",
    "    Obtain the particular stop words (most frequently occurring\n",
    "    words) in the sample, which may differ from those in a list\n",
    "    of generic stop words.\n",
    "\n",
    "    Inputs:\n",
    "      - all_tokens (list of lists of str): all tokens\n",
    "      - num_stop_words (int): number of stop words to remove\n",
    "\n",
    "    Returns: list of most common tokens\n",
    "    '''\n",
    "    all_words = list(itertools.chain.from_iterable(all_tokens))\n",
    "    freq_dist = FreqDist(all_words)\n",
    "    stop_words = freq_dist.most_common(num_stop_words)\n",
    "\n",
    "    return [word[0] for word in stop_words]\n",
    "\n",
    "\n",
    "def make_ngrams(tokens, n):\n",
    "    '''\n",
    "    Take the list of words from a single review and create n-grams.\n",
    "\n",
    "    Input:\n",
    "      - text (list of str): list of processed words in a review\n",
    "      - n (int): maximum number of words per n-gram\n",
    "\n",
    "    Returns: list of 1- to n-word strings for a single review\n",
    "    '''\n",
    "    ngrams = []\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        ngrams += [' '.join(tuple(tokens[j:j+i]))\n",
    "                   for j in range(len(tokens) - i + 1)]\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "# Processing Stage\n",
    "def count_tokens(tokens):\n",
    "    '''\n",
    "    Count each distinct token (entity) in a list of tokens.\n",
    "\n",
    "    Inputs:\n",
    "      - tokens (list of str): list of tokens\n",
    "\n",
    "    Returns: dict mapping tokens to counts\n",
    "    '''\n",
    "    rv = {}\n",
    "\n",
    "    for tok in tokens:\n",
    "        rv[tok] = rv.get(tok, 0) + 1\n",
    "\n",
    "    return rv\n",
    "\n",
    "\n",
    "def compute_idf(docs):\n",
    "    '''\n",
    "    Calculate the inverse document frequency (idf) for each\n",
    "    token in a collection of documents (D), where\n",
    "        idf(t, D) = log(total number of documents in D / \n",
    "                        number of documents containing t).\n",
    "\n",
    "    Inputs:\n",
    "      - docs (list of list of str): list of lists of tokens\n",
    "\n",
    "    Returns: dict mapping terms to idf values\n",
    "    '''\n",
    "    num_docs = len(docs)\n",
    "\n",
    "    idf_dict = {}\n",
    "    docs_set = [set(doc) for doc in docs]\n",
    "    tokens = set.union(*docs_set)\n",
    "\n",
    "    for token in tokens:\n",
    "        docs_with_token = sum([1 for doc in docs_set\n",
    "                               if (token in doc)])\n",
    "        idf_dict[token] = math.log(num_docs / docs_with_token)\n",
    "\n",
    "    return idf_dict\n",
    "\n",
    "\n",
    "# Vectorizing Stage\n",
    "def tfidf_vectorize(revs):\n",
    "    '''\n",
    "    Calculate the tf_idf for each term per document in a collection\n",
    "    of documents. By definition,\n",
    "        tf = 0.5 + 0.5 * (freq_of_term_in_doc / max_freq_in_doc)\n",
    "    and\n",
    "        tf_idf = tf * idf.\n",
    "\n",
    "    Inputs:\n",
    "      - list of lists of strings (list of str): collection of reviews\n",
    "\n",
    "    Returns: DataFrame (tf_idf) and dict (idf)\n",
    "    '''\n",
    "    token_to_freq_by_rev = [count_tokens(rev) for rev in revs]\n",
    "    idf = compute_idf(revs)\n",
    "\n",
    "    for rev in token_to_freq_by_rev:\n",
    "        max_freq = max(rev.values())\n",
    "        for token in rev:\n",
    "            tf = 0.5 + 0.5 * (rev[token] / max_freq)\n",
    "            rev[token] = tf * idf[token]\n",
    "\n",
    "    return pd.DataFrame(token_to_freq_by_rev).fillna(0), idf\n",
    "\n",
    "\n",
    "def get_df_idf_stops(csv_file, n=2, lemmatized=True,\n",
    "                     num_stop_words=20):\n",
    "    '''\n",
    "    Given a dataframe with two columns, rating and text, generate a\n",
    "    dataframe that vectorizes the text, and join it back with the\n",
    "    rating column.\n",
    "\n",
    "    Inputs: \n",
    "        csv_file (str): CSV file name\n",
    "        n (int): range of n-grams to use\n",
    "        lemmatized (bool): whether or not to lemmatize words\n",
    "        num_stop_words (int): number of stop words to remove\n",
    "\n",
    "    Returns: DataFrame, dict (idf), and list (stop words)\n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv(csv_file, usecols=[0, 1],\n",
    "                     names=[\"Rating\", \"Text\"], header=None)\n",
    "    all_tokens = [processing(text, lemmatized) for text in df.Text]\n",
    "\n",
    "    if num_stop_words > 0:\n",
    "        stop_words = get_stop_words(all_tokens, num_stop_words)\n",
    "        all_tokens = [[token for token in tokens if token not in stop_words]\n",
    "                      for tokens in all_tokens]\n",
    "\n",
    "    ngrams = [make_ngrams(tokens, n) for tokens in all_tokens]\n",
    "\n",
    "    final_df, idf = tfidf_vectorize(ngrams)\n",
    "    y_values = df.Rating.astype(\"category\")\n",
    "\n",
    "    final_df[\"Rating\"] = y_values\n",
    "\n",
    "    if num_stop_words > 0:\n",
    "        return final_df, idf, stop_words\n",
    "    else:\n",
    "        return final_df, idf, None  # Should we return an empty list here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_and_scrape.py\n",
    "\n",
    "import urllib.parse\n",
    "import requests\n",
    "import os\n",
    "import bs4\n",
    "import urllib3\n",
    "import certifi\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "MAIN_URL = \"https://www.yelp.com\"\n",
    "\n",
    "\n",
    "def read_url(my_url):\n",
    "    '''\n",
    "    Load HTML from URL. Return result or empty string if the\n",
    "    read fails.\n",
    "\n",
    "    Inputs:\n",
    "      - my_url (str): URL\n",
    "\n",
    "    Returns: str\n",
    "    '''\n",
    "\n",
    "    pm = urllib3.PoolManager(\n",
    "        cert_reqs='CERT_REQUIRED',\n",
    "        ca_certs=certifi.where())\n",
    "\n",
    "    return pm.urlopen(url=my_url, method=\"GET\").data\n",
    "\n",
    "\n",
    "def is_absolute_url(url):\n",
    "    '''\n",
    "    Determine if a URL is an absolute URL.\n",
    "\n",
    "    Inputs:\n",
    "      - url (str): URL\n",
    "\n",
    "    Returns: Bool\n",
    "    '''\n",
    "\n",
    "    if url == \"\":\n",
    "        return False\n",
    "    return urllib.parse.urlparse(url).netloc != \"\"\n",
    "\n",
    "\n",
    "def convert_if_relative_url(new_url, main_url=MAIN_URL):\n",
    "    '''\n",
    "    Attempt to determine whether new_url is a relative URL and if so,\n",
    "    use current_url to determine the path and create a new absolute\n",
    "    URL. Add the protocol, if that is all that is missing.\n",
    "\n",
    "    Inputs:\n",
    "      - new_url (str): the path to the restaurants\n",
    "      - main_url (str): absolute URL\n",
    "\n",
    "    Returns: str or None\n",
    "\n",
    "    Examples:\n",
    "        convert_if_relative_url(\"/biz/girl-and-the-goat-chicago\",\n",
    "                                \"https://www.yelp.com\")\n",
    "        yields \"https://www.yelp.com/biz/girl-and-the-goat-chicago\"\n",
    "    '''\n",
    "    if new_url == \"\" or not is_absolute_url(main_url):\n",
    "        return None\n",
    "\n",
    "    if is_absolute_url(new_url):\n",
    "        return new_url\n",
    "\n",
    "    parsed_url = urllib.parse.urlparse(new_url)\n",
    "    path_parts = parsed_url.path.split(\"/\")\n",
    "\n",
    "    if len(path_parts) == 0:\n",
    "        return None\n",
    "\n",
    "    ext = path_parts[0][-4:]\n",
    "    if ext in [\".edu\", \".org\", \".com\", \".net\"]:\n",
    "        return \"http://\" + new_url\n",
    "    else:\n",
    "        return urllib.parse.urljoin(main_url, new_url)\n",
    "\n",
    "\n",
    "# Crawling and scraping functions\n",
    "def get_total_reviews(soup, counter):\n",
    "    '''\n",
    "    Given a soup object representing a page, obtain the total\n",
    "    number of reviews to help the program determine how many\n",
    "    pages of reviews to scrape.\n",
    "\n",
    "    Inputs:\n",
    "      - soup (bs4 object): soup object\n",
    "      - counter (int): if the program gets blocked by Yelp,\n",
    "                       how many times should it try again\n",
    "                       before giving up and skipping (higher\n",
    "                       number corresponds to longer run-time\n",
    "                       but fewer pages skipped)\n",
    "\n",
    "    Returns: (int) total number reviews for a restaurant\n",
    "    '''\n",
    "    tag = soup.find(\"script\", type=\"application/ld+json\")\n",
    "\n",
    "    # Try again if tag cannot be found;\n",
    "    # number of tries depends on counter\n",
    "    if not tag:\n",
    "        for _ in range(counter):\n",
    "            tag = soup.find(\"script\", type=\"application/ld+json\")\n",
    "            time.sleep(random.randint(1, 3))\n",
    "            if tag:\n",
    "                break\n",
    "        if not tag:\n",
    "            return None\n",
    "\n",
    "    json_object = json.loads(tag.contents[0])\n",
    "    total_reviews = json_object[\"aggregateRating\"]['reviewCount']\n",
    "\n",
    "    return total_reviews\n",
    "\n",
    "\n",
    "def get_reviews_from_page(url, writer, counter):\n",
    "    '''\n",
    "    Given a URL and CSV writer object, write all the reviews\n",
    "    from a given page to the CSV file.\n",
    "\n",
    "    Inputs: \n",
    "      - url (str): URL\n",
    "      - writer (CSV writer object): CSV writer\n",
    "      - counter (int): if the program gets blocked by Yelp,\n",
    "                       how many times should it try again\n",
    "                       before giving up and skipping (higher\n",
    "                       number corresponds to longer run-time\n",
    "                       but fewer pages skipped)\n",
    "                         \n",
    "    Returns: None, modifies the CSV file in place\n",
    "    '''\n",
    "\n",
    "    html = read_url(url)\n",
    "    soup = bs4.BeautifulSoup(html, \"lxml\")\n",
    "    tag = soup.find(\"script\", type=\"application/ld+json\")\n",
    "\n",
    "    # Tries again if tag cannot be found, number of tries depend on counter\n",
    "    if not tag:\n",
    "        for _ in range(counter):\n",
    "            tag = soup.find(\"script\", type=\"application/ld+json\")\n",
    "            time.sleep(random.randint(1, 3))\n",
    "            if tag:\n",
    "                break\n",
    "        if not tag:\n",
    "            print(\"Failure at page \" + str(url))\n",
    "            return None\n",
    "\n",
    "    print(\"Success at page \" + str(url))\n",
    "\n",
    "    json_object = json.loads(tag.contents[0])\n",
    "    reviews = json_object[\"review\"]\n",
    "\n",
    "    for review in reviews:\n",
    "        rating = review['reviewRating'][\"ratingValue\"]\n",
    "        text = review[\"description\"]\n",
    "        row = [rating, text]\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "def crawl_resto(url, writer, counter):\n",
    "    '''\n",
    "    Crawl the restaurant and get all reviews from the restaurant\n",
    "\n",
    "    Inputs:\n",
    "      - url (str): URL\n",
    "      - writer (csv writer): writer object\n",
    "      - counter (int): if the program gets blocked by Yelp,\n",
    "                       how many times should it try again\n",
    "                       before giving up and skipping (higher\n",
    "                       number corresponds to longer run-time\n",
    "                       but fewer pages skipped)\n",
    "\n",
    "    Returns: None, modifies the CSV file in place\n",
    "    '''\n",
    "    html = read_url(url)\n",
    "    soup = bs4.BeautifulSoup(html, \"lxml\")\n",
    "    total_reviews = get_total_reviews(soup, counter)\n",
    "\n",
    "    if not total_reviews:\n",
    "        print('failure at this restaurant ' + str(url))\n",
    "        return None\n",
    "\n",
    "    print('sucess at this restaurant' + str(url))\n",
    "\n",
    "    review_pages = []\n",
    "\n",
    "    # Each page has 20 reviews, so we increment by 20\n",
    "    for i in range(0, total_reviews, 20):\n",
    "        review_pages.append(url + \"?start=\" + str(i))\n",
    "\n",
    "    for review_page in review_pages:\n",
    "        get_reviews_from_page(review_page, writer, counter)\n",
    "\n",
    "        # Random sleep to avoid being banned by Yelp\n",
    "        time.sleep(random.randint(1, 3))\n",
    "\n",
    "\n",
    "def get_links_from_page(url):\n",
    "    '''\n",
    "    Given a URL, scrape all other URLs that refer to restaurant\n",
    "    home pages, and convert it to an absolute URL.\n",
    "\n",
    "    Inputs: \n",
    "      - url (str): URL\n",
    "\n",
    "    Returns: set of restaurant links from the page\n",
    "    '''\n",
    "    html = read_url(url)\n",
    "    soup = bs4.BeautifulSoup(html, \"lxml\")\n",
    "    all_tags = soup.find_all(\"a\", href=True)\n",
    "    all_links = [tag.get(\"href\") for tag in all_tags]\n",
    "    good_links = {convert_if_relative_url(link) for link\n",
    "                  in all_links if link.startswith('/biz')\n",
    "                  and \"?\" not in link}\n",
    "\n",
    "    return good_links\n",
    "\n",
    "\n",
    "def crawl_city(city_url):\n",
    "    '''\n",
    "    Crawl a city and get all the URLs of restaurants within\n",
    "    the city.\n",
    "\n",
    "    Inputs:\n",
    "      - city_url (str): URL of the city's page on Yelp\n",
    "\n",
    "    Returns: list of restaurant links in city\n",
    "    '''\n",
    "    html = read_url(url)\n",
    "    soup = bs4.BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    # Yelp displays 24 pages of reviews for each location\n",
    "    total_restos = 24\n",
    "    resto_pages = []\n",
    "\n",
    "    # Each page has 10 restaurants, so we increment by 10\n",
    "    for i in range(0, total_restos, 10):\n",
    "        resto_pages.append(city_url + \"&start=\" + str(i))\n",
    "\n",
    "    city_restos = []\n",
    "    for resto_page in resto_pages:\n",
    "        city_restos += get_links_from_page(resto_page)\n",
    "        time.sleep(random.randint(1, 3))  # Random sleep to avoid\n",
    "                                          # being banned by Yelp\n",
    "\n",
    "    return city_restos\n",
    "\n",
    "\n",
    "def crawl_and_scrape(counter=10,\n",
    "                     city_url=(\"https://www.yelp.com/\"\"\n",
    "                               \"search?find_desc=&\"\n",
    "                               \"find_loc=Chicago%2C%20IL\"),\n",
    "                     csv_repo=\"scraped_data/\"):\n",
    "    '''\n",
    "    Crawl the city of Chicago, unless another city url is given,\n",
    "    and export all reviews from restaurants in that city to a CSV\n",
    "    file. CSV file does not contain headers.\n",
    "\n",
    "    Inputs:\n",
    "      - counter (int): if the program gets blocked by Yelp,\n",
    "                       how many times should it try again\n",
    "                       before giving up and skipping (higher\n",
    "                       number corresponds to longer run-time\n",
    "                       but fewer pages skipped)\n",
    "      - city_url (str): Yelp URL of the city\n",
    "      - csv_repo (str): name of repository in which to store\n",
    "                        scraped data\n",
    "\n",
    "    Returns: None, writes a CSV file\n",
    "    '''\n",
    "    city_restos = crawl_city(city_url)\n",
    "\n",
    "    for i, resto in enumerate(city_restos):\n",
    "        filename = csv_repo + str(i) + \".csv\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            csvwriter = csv.writer(f)\n",
    "            crawl_resto(resto, csvwriter, counter)\n",
    "            # Random sleep to avoid being banned by Yelp\n",
    "            time.sleep(random.randint(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_data.py\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "# Goal: Collect 10,000 reviews, with 2,000 reviews in each category\n",
    "\n",
    "\n",
    "def merge_data(scraped_data_dir='scraped_data/', num_samples=10000,\n",
    "               random_state=1234, write_to_csv=True):\n",
    "    '''\n",
    "    First, combine all restaurant reviews into one dataframe.\n",
    "    Then, group each review by rating and randomly sample 2,000\n",
    "    reviews from each group. Header argument is determined if\n",
    "    there is header column in each CSV file from scraped data.\n",
    "\n",
    "    Return: DataFrame with equal distribution of ratings\n",
    "    '''\n",
    "    # Get a list of all CSV filenames in scraped data directory\n",
    "    all_rest_csv = [scraped_data_dir + file_name for file_name\n",
    "                    in os.listdir(scraped_data_dir)\n",
    "                    if file_name.endswith('.csv')]\n",
    "\n",
    "    # Concatenate all DataFrames together\n",
    "    df_from_each_file = (pd.read_csv(f) for f in all_rest_csv)\n",
    "    concatenated_df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "    # Select (num_samples / 5) from each rating group\n",
    "    num_samples_per_rating = round(num_samples/5)\n",
    "    concat_data = (concatenated_df.groupby(\"Rating\")\n",
    "                   .sample(n=num_samples_per_rating,\n",
    "                           random_state=random_state)\n",
    "                   .reset_index(drop=True))\n",
    "    \n",
    "    # Write concat_data to CSV\n",
    "    concat_data.to_csv('merged_data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import linear_model, tree, neighbors\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from nltk import pos_tag\n",
    "# from analyze_words import *\n",
    "import itertools\n",
    "\n",
    "\n",
    "def applyModels(model, x_train, y_train):\n",
    "    '''\n",
    "    Fit a model to a pair of x and y training data.\n",
    "    \n",
    "    Inputs:\n",
    "      - model (Model): model being fitted\n",
    "      - x_train (arr): x training data\n",
    "      - y_train (arr): y training data\n",
    "    \n",
    "    Returns: Model\n",
    "    '''\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predictModel(model, x_test):\n",
    "    '''\n",
    "    Use a model to generate a prediction for y from x testing data.\n",
    "    \n",
    "    Inputs:\n",
    "      - model (Model): model being applied\n",
    "      - x_test (arr): x testing data\n",
    "    \n",
    "    Returns: arr\n",
    "    '''\n",
    "    prediction = model.predict(x_test)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def evaluateModel(prediction, y_test):\n",
    "    '''\n",
    "    Calculate the accuracy of a model based on the proportion of\n",
    "    accurate predictions using the testing data. Accuracy is\n",
    "    weighted by the deviance from the actual rating.\n",
    "    \n",
    "    Inputs:\n",
    "      - prediction (arr): predicted y values\n",
    "      - y_test (arr): actual y values\n",
    "    \n",
    "    Returns: float\n",
    "    '''\n",
    "    # Convert into DataFrame for easier handling\n",
    "    pred_test_df = pd.DataFrame({'predict': prediction,\n",
    "                                 'actual': y_test}).astype('int')\n",
    "    pred_test_df['difference'] = (pred_test_df.predict\n",
    "                                  - pred_test_df.actual).abs()\n",
    "\n",
    "    num_tests = len(pred_test_df.index)\n",
    "    total_deviance = pred_test_df['difference'].sum()\n",
    "\n",
    "    # Maximum deviance is 4 (5-star rating vs. 1-star rating)\n",
    "    weighted_accuracy = 1 - (total_deviance / (4 * num_tests))\n",
    "\n",
    "    return weighted_accuracy\n",
    "\n",
    "\n",
    "def get_weighted_accuracy(x_train, x_test, y_train, y_test, alpha):\n",
    "    '''\n",
    "    Calculate weighted accuracy of a model.\n",
    "    \n",
    "    Inputs:\n",
    "      - x_train (arr): x training data\n",
    "      - x_test (arr): x testing data\n",
    "      - y_train (arr): y training data\n",
    "      - y_test (arr): y testing data\n",
    "      - alpha (float): constant that multiplies regularization term\n",
    "      \n",
    "    Returns: float\n",
    "    '''\n",
    "    model = linear_model.SGDClassifier(alpha=alpha)\n",
    "    trained_model = applyModels(model, x_train, y_train)\n",
    "    prediction = predictModel(trained_model, x_test)\n",
    "    weighted_accuracy = evaluateModel(prediction, y_test)\n",
    "\n",
    "    return weighted_accuracy\n",
    "\n",
    "\n",
    "def transformFeatureSelection(model, x):\n",
    "    # Need doc string\n",
    "    return model.transform(x)\n",
    "\n",
    "\n",
    "def applyFeatureSelection(model, x_train, y_train):\n",
    "    # Need doc string\n",
    "    model = model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_additional_features():\n",
    "    # In case we want to include number of taggings\n",
    "    # (like verb, noun) to our columns\n",
    "    pass\n",
    "\n",
    "\n",
    "def optimize_model(csv_file, testing_fraction=0.95):\n",
    "    '''\n",
    "    Find the optimal combination of parameters (maximum n-gram length,\n",
    "    whether to lemmatize, number of stop words, and alpha) for the\n",
    "    suggested star rating model, as well as the corresponding DataFrame, \n",
    "    idf dictionary, and list of stop words.\n",
    "    \n",
    "    Inputs:\n",
    "      - csv_file (string): CSV file name\n",
    "      - testing_fraction (float): proportion of data reserved for testing\n",
    "    \n",
    "    Returns: DataFrame, dict (parameters), dict (idf), list of str\n",
    "    '''\n",
    "    # Combinations\n",
    "    ngrams = [1, 2, 3, 4, 5]\n",
    "    lemmatizes = [True, False]\n",
    "    stop_words = [0, 10, 20]\n",
    "    alphas = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "    all_combi = list(itertools.product(ngrams, lemmatizes, stop_words, alphas))\n",
    "\n",
    "    max_accuracy = -1\n",
    "    best_combi = None\n",
    "    best_idf = None\n",
    "    best_df = None\n",
    "    best_stop = None\n",
    "\n",
    "    print(\"Completed initializing.\")\n",
    "\n",
    "    for combi in all_combi:\n",
    "        ngram, lemmatize, stop_word, alpha = combi\n",
    "        df, idf, chosen_stops = get_df_idf_stops(csv_file, n=ngram,\n",
    "                                                 lemmatized=lemmatize,\n",
    "                                                 num_stop_words=stop_word)\n",
    "        (x_train, x_test,\n",
    "         y_train, y_test) = train_test_split(df.drop(\"Rating\", axis=1),\n",
    "                                             df.Rating,\n",
    "                                             test_size=testing_fraction,\n",
    "                                             random_state=33)\n",
    "        weighted_accuracy = get_weighted_accuracy(x_train, x_test,\n",
    "                                                  y_train, y_test, alpha)\n",
    "\n",
    "        print(combi, \"Finished testing. | Accuracy: \", weighted_accuracy)\n",
    "\n",
    "        if weighted_accuracy > max_accuracy:\n",
    "            max_accuracy = weighted_accuracy\n",
    "            best_combi = combi\n",
    "            best_idf = idf\n",
    "            best_df = df\n",
    "            best_stop = chosen_stops\n",
    "\n",
    "    best_combi_dict = {\"ngram\": best_combi[0], \"lemmatize\": best_combi[1],\n",
    "                       \"stop_word\": best_combi[2], \"alpha\": best_combi[3]}\n",
    "\n",
    "    return best_df, best_combi_dict, best_idf, best_stop\n",
    "\n",
    "\n",
    "def main_modelling(csv_file, testing_fraction=0.95):\n",
    "    '''\n",
    "    Generate the optimal model for predicting Yelp review ratings by\n",
    "    cycling through combinations of parameters and save it as a PKL file.\n",
    "    \n",
    "    Inputs:\n",
    "      - csv_file (string): CSV file name\n",
    "      - testing_fraction (float): proportion of data reserved for testing\n",
    "    \n",
    "    Returns: None, writes PKL file\n",
    "    '''\n",
    "    # Input and Model Tuning\n",
    "    df, comb, idf, stop = optimize_model(csv_file, testing_fraction)\n",
    "\n",
    "    (x_train, x_test,\n",
    "     y_train, y_test) = train_test_split(df.drop(\"Rating\", axis=1),\n",
    "                                         df.Rating,\n",
    "                                         test_size=testing_fraction,\n",
    "                                         random_state=33)\n",
    "\n",
    "    # Feature Selection\n",
    "    model = linear_model.SGDClassifier(alpha=comb[\"alpha\"])\n",
    "    trained_model = applyModels(model, x_train, y_train)\n",
    "    feature_selection_model = SelectFromModel(trained_model)\n",
    "    trained_feature_selection_model = applyFeatureSelection(feature_selection_model,\n",
    "                                                            x_train, y_train)\n",
    "    x_train = transformFeatureSelection(trained_feature_selection_model,\n",
    "                                        x_train)\n",
    "    x_test = transformFeatureSelection(trained_feature_selection_model,\n",
    "                                       x_test)\n",
    "\n",
    "    final_model = applyModels(model, x_train, y_train)\n",
    "    prediction = predictModel(final_model, x_test)\n",
    "\n",
    "    print(\"Final Model Classification Report\")\n",
    "    print(classification_report(prediction, y_test))\n",
    "    print(\"Accuracy Score\")\n",
    "    print(evaluateModel(prediction, y_test))\n",
    "\n",
    "    # Save best Model\n",
    "    joblib.dump(final_model, \"optimal_args/final_model.pkl\")\n",
    "\n",
    "    # Save best columns, idf, combination, and stop words\n",
    "    feature_idx = trained_feature_selection_model.get_support()\n",
    "    column_names = df.drop(\"Rating\", axis=1).columns[feature_idx]\n",
    "    with open('optimal_args/columns.json', 'w') as f:\n",
    "        json.dump(list(column_names), f)\n",
    "    with open('optimal_args/idf.json', 'w') as f:\n",
    "        json.dump(idf, f)\n",
    "    with open('optimal_args/combination.json', 'w') as f:\n",
    "        json.dump(comb, f)\n",
    "    with open('optimal_args/stop_words.json', 'w') as f:\n",
    "        json.dump(stop, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "# from analyze_words import *\n",
    "# from model import *\n",
    "import sys\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "\n",
    "def user_interface():\n",
    "    '''Prompt user to input a review, and suggest a star rating.'''\n",
    "    print(\"==================================================\")\n",
    "    print(\"   Welcome to the Suggested Star Rating System!\")\n",
    "    print()\n",
    "    print(\"            Copy and paste your review.\")\n",
    "    print()\n",
    "    print(\"       Type Control-D to exit the program.\")\n",
    "    print(\"==================================================\")\n",
    "    print()\n",
    "    try:\n",
    "        while True:\n",
    "            review = input(\"Enter review here: \")\n",
    "            review = str(review)\n",
    "            if len(review) >= 50:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please input a longer review.\")\n",
    "\n",
    "        x_array = process_input(review)\n",
    "        final_model = joblib.load(\"perfect_model.pkl\")\n",
    "        prediction = predictModel(final_model, [x_array])\n",
    "        star_rating = int(prediction)\n",
    "\n",
    "        print(\"Your suggested star rating is: {}\".format(star_rating))\n",
    "        print(\"Thank you for using our Suggested Star Rating System!\")\n",
    "    except EOFError:\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "def process_input(user_input):\n",
    "    '''\n",
    "    Convert a review input by the user into an array of zeros,\n",
    "    where each item corresponding to a valid n-gram in the input\n",
    "    is replaced by the n-gram's tfidf. This allows a review to be\n",
    "    evaluated by a model.\n",
    "    \n",
    "    Inputs:\n",
    "      - user_input (str): review input by user\n",
    "      \n",
    "    Returns: arr\n",
    "    '''\n",
    "    with open(\"columns.json\") as f:\n",
    "        columns = json.load(f)\n",
    "    with open(\"idf.json\") as f:\n",
    "        idf = json.load(f)\n",
    "    with open(\"combination.json\") as f:\n",
    "        comb = json.load(f)\n",
    "    with open(\"stop_words.json\") as f:\n",
    "        stop_words = json.load(f)\n",
    "\n",
    "    processed_input = processing(user_input, comb[\"lemmatize\"])\n",
    "\n",
    "    if comb['stop_word'] > 0:\n",
    "        processed_input = [token for token in processed_input\n",
    "                           if token not in stop_words]\n",
    "\n",
    "    ngrams = make_ngrams(processed_input, comb[\"ngram\"])\n",
    "    tf = compute_tf(ngrams)\n",
    "\n",
    "    ngrams_set = set(ngrams)\n",
    "    columns_set = set(columns)\n",
    "    indices = pd.Index(columns)\n",
    "\n",
    "    x_array = np.zeros(len(columns))\n",
    "\n",
    "    for token in ngrams_set:\n",
    "        if token in columns_set:\n",
    "            tfidf = tf[token] * idf[token]\n",
    "            index = indices.get_loc(token)\n",
    "            x_array[index] = tfidf\n",
    "\n",
    "    return x_array\n",
    "\n",
    "\n",
    "def compute_tf(doc):\n",
    "    '''\n",
    "    Compute the augmented term frequency (tf) of the tokens\n",
    "    in a document.\n",
    "\n",
    "    Inputs: \n",
    "      - doc (list of str): a list of tokens\n",
    "\n",
    "    Returns: dict mapping terms to tf values\n",
    "    '''\n",
    "    token_dict = count_tokens(doc)\n",
    "    tf_dict = {}\n",
    "    max_count = max(token_dict.values())\n",
    "\n",
    "    for token, count in token_dict.items():\n",
    "        tf_dict[token] = 0.5 + 0.5 * (count / max_count)\n",
    "\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, idf, stops = get_df_idf_stops(\"merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gross',\n",
       " 'bloody',\n",
       " 'burgers',\n",
       " 'three',\n",
       " 'heady',\n",
       " 'beers',\n",
       " 'blaring',\n",
       " 'music',\n",
       " 'ears',\n",
       " 'still',\n",
       " 'left',\n",
       " 'tip',\n",
       " 'asked',\n",
       " 'medium',\n",
       " 'got',\n",
       " 'masks',\n",
       " 'everytime',\n",
       " 'you',\n",
       " 'talk',\n",
       " 'server',\n",
       " 'so',\n",
       " 'if',\n",
       " \"don't\",\n",
       " 'want',\n",
       " 'loud',\n",
       " 'metal',\n",
       " 'or',\n",
       " 'mask',\n",
       " 'then',\n",
       " 'skip',\n",
       " 'place',\n",
       " 'gross bloody',\n",
       " 'bloody burgers',\n",
       " 'burgers three',\n",
       " 'three heady',\n",
       " 'heady beers',\n",
       " 'beers blaring',\n",
       " 'blaring music',\n",
       " 'music ears',\n",
       " 'ears still',\n",
       " 'still left',\n",
       " 'left tip',\n",
       " 'tip asked',\n",
       " 'asked medium',\n",
       " 'medium got',\n",
       " 'got bloody',\n",
       " 'bloody tip',\n",
       " 'tip masks',\n",
       " 'masks everytime',\n",
       " 'everytime you',\n",
       " 'you talk',\n",
       " 'talk server',\n",
       " 'server so',\n",
       " 'so if',\n",
       " 'if you',\n",
       " \"you don't\",\n",
       " \"don't want\",\n",
       " 'want bloody',\n",
       " 'burgers loud',\n",
       " 'loud metal',\n",
       " 'metal music',\n",
       " 'music or',\n",
       " 'or mask',\n",
       " 'mask everytime',\n",
       " 'everytime then',\n",
       " 'then skip',\n",
       " 'skip place',\n",
       " 'update',\n",
       " 'after',\n",
       " 'two',\n",
       " 'weeks',\n",
       " 'have',\n",
       " 'received',\n",
       " 'promised',\n",
       " 'refund',\n",
       " 'item',\n",
       " 'out',\n",
       " 'order',\n",
       " 'make',\n",
       " 'right',\n",
       " \"pequod's\",\n",
       " 'almost',\n",
       " 'hours',\n",
       " 'waiting',\n",
       " 'our',\n",
       " 'delivery',\n",
       " 'instead',\n",
       " 'appetizer',\n",
       " 'sampler',\n",
       " 'ordered',\n",
       " 'an',\n",
       " 'cheese',\n",
       " 'bread',\n",
       " 'called',\n",
       " 'store',\n",
       " 'person',\n",
       " 'who',\n",
       " 'answered',\n",
       " 'stated',\n",
       " 'she',\n",
       " 'going',\n",
       " 'contact',\n",
       " 'driver',\n",
       " 'see',\n",
       " 'he',\n",
       " 'had',\n",
       " 'correct',\n",
       " 'would',\n",
       " 'call',\n",
       " 'me',\n",
       " 'back',\n",
       " 'ate',\n",
       " 'pizza',\n",
       " 'more',\n",
       " 'on',\n",
       " 'later',\n",
       " 'never',\n",
       " 'heard',\n",
       " 'from',\n",
       " 'her',\n",
       " 'missing',\n",
       " 'which',\n",
       " 'said',\n",
       " 'initiate',\n",
       " 'tasted',\n",
       " 'okay--good',\n",
       " 'sauce',\n",
       " 'flavor',\n",
       " 'crust',\n",
       " 'okay',\n",
       " 'really',\n",
       " 'burned',\n",
       " 'edges',\n",
       " 'gave',\n",
       " 'bitter',\n",
       " 'unfortunate',\n",
       " 'because',\n",
       " 'normally',\n",
       " \"i'm\",\n",
       " 'eater',\n",
       " 'reading',\n",
       " 'other',\n",
       " 'reviews',\n",
       " 'apparently',\n",
       " 'intentionally',\n",
       " 'maybe',\n",
       " 'good',\n",
       " 'some',\n",
       " 'folks',\n",
       " 'jam',\n",
       " 'visiting',\n",
       " 'town',\n",
       " 'will',\n",
       " 'most',\n",
       " 'likely',\n",
       " 'be',\n",
       " 'first',\n",
       " 'only',\n",
       " 'experience',\n",
       " 'update after',\n",
       " 'after two',\n",
       " 'two weeks',\n",
       " 'weeks still',\n",
       " 'still have',\n",
       " 'have received',\n",
       " 'received promised',\n",
       " 'promised refund',\n",
       " 'refund item',\n",
       " 'item left',\n",
       " 'left out',\n",
       " 'out order',\n",
       " 'order make',\n",
       " 'make right',\n",
       " \"right pequod's\",\n",
       " \"pequod's after\",\n",
       " 'after almost',\n",
       " 'almost two',\n",
       " 'two hours',\n",
       " 'hours waiting',\n",
       " 'waiting received',\n",
       " 'received our',\n",
       " 'our delivery',\n",
       " 'delivery instead',\n",
       " 'instead appetizer',\n",
       " 'appetizer sampler',\n",
       " 'sampler ordered',\n",
       " 'ordered received',\n",
       " 'received an',\n",
       " 'an order',\n",
       " 'order cheese',\n",
       " 'cheese bread',\n",
       " 'bread called',\n",
       " 'called store',\n",
       " 'store person',\n",
       " 'person who',\n",
       " 'who answered',\n",
       " 'answered stated',\n",
       " 'stated she',\n",
       " 'she going',\n",
       " 'going contact',\n",
       " 'contact driver',\n",
       " 'driver see',\n",
       " 'see if',\n",
       " 'if he',\n",
       " 'he had',\n",
       " 'had our',\n",
       " 'our correct',\n",
       " 'correct item',\n",
       " 'item she',\n",
       " 'she would',\n",
       " 'would call',\n",
       " 'call me',\n",
       " 'me back',\n",
       " 'back ate',\n",
       " 'ate pizza',\n",
       " 'pizza ordered',\n",
       " 'ordered more',\n",
       " 'more on',\n",
       " 'on later',\n",
       " 'later never',\n",
       " 'never heard',\n",
       " 'heard back',\n",
       " 'back from',\n",
       " 'from her',\n",
       " 'her called',\n",
       " 'called back',\n",
       " 'back asked',\n",
       " 'asked refund',\n",
       " 'refund on',\n",
       " 'on missing',\n",
       " 'missing item',\n",
       " 'item which',\n",
       " 'which she',\n",
       " 'she said',\n",
       " 'said she',\n",
       " 'would initiate',\n",
       " 'initiate pizza',\n",
       " 'pizza tasted',\n",
       " 'tasted okay--good',\n",
       " 'okay--good cheese',\n",
       " 'cheese sauce',\n",
       " 'sauce flavor',\n",
       " 'flavor crust',\n",
       " 'crust okay',\n",
       " 'okay really',\n",
       " 'really burned',\n",
       " 'burned on',\n",
       " 'on edges',\n",
       " 'edges gave',\n",
       " 'gave bitter',\n",
       " 'bitter flavor',\n",
       " 'flavor which',\n",
       " 'which unfortunate',\n",
       " 'unfortunate because',\n",
       " 'because normally',\n",
       " \"normally i'm\",\n",
       " \"i'm crust\",\n",
       " 'crust eater',\n",
       " 'eater reading',\n",
       " 'reading other',\n",
       " 'other reviews',\n",
       " 'reviews apparently',\n",
       " 'apparently crust',\n",
       " 'crust burned',\n",
       " 'burned intentionally',\n",
       " 'intentionally maybe',\n",
       " 'maybe good',\n",
       " 'good some',\n",
       " 'some folks',\n",
       " 'folks jam',\n",
       " \"jam i'm\",\n",
       " \"i'm visiting\",\n",
       " 'visiting from',\n",
       " 'from out',\n",
       " 'out town',\n",
       " 'town so',\n",
       " 'so will',\n",
       " 'will most',\n",
       " 'most likely',\n",
       " 'likely be',\n",
       " 'be first',\n",
       " 'first only',\n",
       " 'only experience',\n",
       " \"experience pequod's\",\n",
       " 'whoever',\n",
       " 'blonde',\n",
       " 'curly',\n",
       " 'hair',\n",
       " 'working',\n",
       " 'there',\n",
       " 'are',\n",
       " 'incredibly',\n",
       " 'rude',\n",
       " 'great',\n",
       " 'establishment',\n",
       " 'continue',\n",
       " 'feel',\n",
       " 'very',\n",
       " 'uncomfortable',\n",
       " 'their',\n",
       " 'level',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'act',\n",
       " 'like',\n",
       " 'child',\n",
       " 'whoever person',\n",
       " 'person blonde',\n",
       " 'blonde curly',\n",
       " 'curly hair',\n",
       " 'hair working',\n",
       " 'working there',\n",
       " 'there are',\n",
       " 'are incredibly',\n",
       " 'incredibly rude',\n",
       " 'rude great',\n",
       " 'great establishment',\n",
       " 'establishment want',\n",
       " 'want continue',\n",
       " 'continue going',\n",
       " 'going feel',\n",
       " 'feel very',\n",
       " 'very uncomfortable',\n",
       " 'uncomfortable their',\n",
       " 'their level',\n",
       " 'level customer',\n",
       " 'customer service',\n",
       " 'service act',\n",
       " 'act like',\n",
       " 'like child',\n",
       " 'terrible',\n",
       " 'service.no',\n",
       " 'hospitality',\n",
       " 'booked',\n",
       " 'weekend',\n",
       " 'brunch',\n",
       " 'when',\n",
       " 'arrived',\n",
       " 'wait',\n",
       " 'lot',\n",
       " 'until',\n",
       " 'seated',\n",
       " 'waited',\n",
       " 'again',\n",
       " 'order...and',\n",
       " 'checking',\n",
       " 'out.our',\n",
       " 'cold',\n",
       " 'showed',\n",
       " 'no',\n",
       " 'trashes',\n",
       " 'around',\n",
       " 'outdoor',\n",
       " 'seating',\n",
       " 'unpleasant',\n",
       " 'told',\n",
       " 'us',\n",
       " 'waffle',\n",
       " 'schnitzel',\n",
       " 'unavailable',\n",
       " 'at',\n",
       " 'timing',\n",
       " 'ordering',\n",
       " 'should',\n",
       " 'informed',\n",
       " 'before',\n",
       " 'food',\n",
       " 'also',\n",
       " 'average',\n",
       " 'blunt',\n",
       " 'surprise',\n",
       " 'just',\n",
       " 'much',\n",
       " 'pricey',\n",
       " 'portion',\n",
       " 'charged',\n",
       " 'full',\n",
       " 'price',\n",
       " 'though',\n",
       " 'already',\n",
       " 'put',\n",
       " 'advance',\n",
       " 'online...so',\n",
       " 'ask',\n",
       " 'come',\n",
       " 'terrible service.no',\n",
       " 'service.no hospitality',\n",
       " 'hospitality booked',\n",
       " 'booked weekend',\n",
       " 'weekend brunch',\n",
       " 'brunch when',\n",
       " 'when arrived',\n",
       " 'arrived had',\n",
       " 'had wait',\n",
       " 'wait lot',\n",
       " 'lot until',\n",
       " 'until seated',\n",
       " 'seated then',\n",
       " 'then waited',\n",
       " 'waited again',\n",
       " 'again order...and',\n",
       " 'order...and again',\n",
       " 'again when',\n",
       " 'when checking',\n",
       " 'checking out.our',\n",
       " 'out.our server',\n",
       " 'server cold',\n",
       " 'cold showed',\n",
       " 'showed no',\n",
       " 'no hospitality',\n",
       " 'hospitality there',\n",
       " 'there lot',\n",
       " 'lot trashes',\n",
       " 'trashes around',\n",
       " 'around our',\n",
       " 'our outdoor',\n",
       " 'outdoor seating',\n",
       " 'seating very',\n",
       " 'very very',\n",
       " 'very unpleasant',\n",
       " 'unpleasant our',\n",
       " 'our server',\n",
       " 'server told',\n",
       " 'told us',\n",
       " 'us waffle',\n",
       " 'waffle schnitzel',\n",
       " 'schnitzel unavailable',\n",
       " 'unavailable at',\n",
       " 'at timing',\n",
       " 'timing ordering',\n",
       " 'ordering he',\n",
       " 'he should',\n",
       " 'should have',\n",
       " 'have informed',\n",
       " 'informed us',\n",
       " 'us before',\n",
       " 'before food',\n",
       " 'food also',\n",
       " 'also average',\n",
       " 'average blunt',\n",
       " 'blunt no',\n",
       " 'no surprise',\n",
       " 'surprise just',\n",
       " 'just very',\n",
       " 'very much',\n",
       " 'much pricey',\n",
       " 'pricey their',\n",
       " 'their portion',\n",
       " 'portion charged',\n",
       " 'charged us',\n",
       " 'us full',\n",
       " 'full price',\n",
       " 'price though',\n",
       " 'though already',\n",
       " 'already put',\n",
       " 'put advance',\n",
       " 'advance when',\n",
       " 'when booked',\n",
       " 'booked online...so',\n",
       " 'online...so had',\n",
       " 'had ask',\n",
       " 'ask server',\n",
       " 'server correct',\n",
       " 'correct never',\n",
       " 'never come',\n",
       " 'come back',\n",
       " 'crap',\n",
       " 'been',\n",
       " 'privileged',\n",
       " 'eaten',\n",
       " 'twice',\n",
       " 'via',\n",
       " 'goldbelly',\n",
       " 'caramelized',\n",
       " 'crappy',\n",
       " 'someone',\n",
       " 'needs',\n",
       " 'learn',\n",
       " 'how',\n",
       " 'even',\n",
       " 'italian',\n",
       " 'zero',\n",
       " 'tasteless',\n",
       " 'thousands',\n",
       " 'calories',\n",
       " 'carbs',\n",
       " 'taste',\n",
       " 'understand',\n",
       " 'shipping',\n",
       " 'across',\n",
       " 'country',\n",
       " 'difficult',\n",
       " \"can't\",\n",
       " 'ship',\n",
       " 'deliver',\n",
       " \"doesn't\",\n",
       " 'transform',\n",
       " 'into',\n",
       " 'inedible',\n",
       " 'gummy',\n",
       " 'attack',\n",
       " 'people',\n",
       " 'sheltered',\n",
       " 'suffering',\n",
       " 'taking',\n",
       " 'advantage',\n",
       " 'global',\n",
       " 'pandemic',\n",
       " 'sending',\n",
       " 'something',\n",
       " 'cheap',\n",
       " 'inferior',\n",
       " 'unforgivable',\n",
       " \"pequod's crap\",\n",
       " 'crap pizza',\n",
       " 'pizza have',\n",
       " 'have been',\n",
       " 'been privileged',\n",
       " 'privileged have',\n",
       " 'have eaten',\n",
       " 'eaten there',\n",
       " 'there twice',\n",
       " 'twice ordered',\n",
       " 'ordered via',\n",
       " 'via goldbelly',\n",
       " 'goldbelly crust',\n",
       " 'crust no',\n",
       " 'no cheese',\n",
       " 'cheese no',\n",
       " 'no caramelized',\n",
       " 'caramelized cheese',\n",
       " 'cheese just',\n",
       " 'just crappy',\n",
       " 'crappy bread',\n",
       " 'bread someone',\n",
       " 'someone needs',\n",
       " 'needs learn',\n",
       " 'learn how',\n",
       " 'how make',\n",
       " 'make bread',\n",
       " 'bread or',\n",
       " 'or even',\n",
       " 'even italian',\n",
       " 'italian zero',\n",
       " 'zero tasteless',\n",
       " 'tasteless just',\n",
       " 'just thousands',\n",
       " 'thousands calories',\n",
       " 'calories carbs',\n",
       " 'carbs zero',\n",
       " 'zero taste',\n",
       " 'taste understand',\n",
       " 'understand shipping',\n",
       " 'shipping across',\n",
       " 'across country',\n",
       " 'country difficult',\n",
       " 'difficult most',\n",
       " \"most can't\",\n",
       " \"can't ship\",\n",
       " 'ship deliver',\n",
       " 'deliver across',\n",
       " 'across town',\n",
       " 'town pizza',\n",
       " 'pizza crust',\n",
       " \"crust doesn't\",\n",
       " \"doesn't transform\",\n",
       " 'transform into',\n",
       " 'into an',\n",
       " 'an inedible',\n",
       " 'inedible gummy',\n",
       " 'gummy attack',\n",
       " 'attack on',\n",
       " 'on people',\n",
       " 'people sheltered',\n",
       " 'sheltered because',\n",
       " 'because are',\n",
       " 'are suffering',\n",
       " 'suffering taking',\n",
       " 'taking advantage',\n",
       " 'advantage global',\n",
       " 'global pandemic',\n",
       " 'pandemic sending',\n",
       " 'sending something',\n",
       " 'something cheap',\n",
       " 'cheap an',\n",
       " 'an inferior',\n",
       " 'inferior unforgivable',\n",
       " 'extremely',\n",
       " 'excited',\n",
       " 'go',\n",
       " 'lunch',\n",
       " 'au',\n",
       " 'cheval',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'indoor',\n",
       " 'dining',\n",
       " 'approved',\n",
       " 'know',\n",
       " 'what',\n",
       " 'considered',\n",
       " 'one',\n",
       " 'best',\n",
       " 'all',\n",
       " 'time',\n",
       " \"couldn't\",\n",
       " 'try',\n",
       " 'top',\n",
       " 'hygiene',\n",
       " 'took',\n",
       " 'precautions',\n",
       " 'led',\n",
       " 'seat',\n",
       " 'room',\n",
       " 'frigid',\n",
       " 'assured',\n",
       " 'warm',\n",
       " 'up',\n",
       " 'sadly',\n",
       " 'did',\n",
       " 'sat',\n",
       " 'looked',\n",
       " 'menu',\n",
       " 'pressed',\n",
       " 'entire',\n",
       " 'quite',\n",
       " 'frankly',\n",
       " 'felt',\n",
       " 'rushed',\n",
       " 'pushy',\n",
       " 'started',\n",
       " 'mary',\n",
       " 'bone',\n",
       " 'marrow',\n",
       " 'tasty',\n",
       " 'came',\n",
       " 'caesar',\n",
       " 'salad',\n",
       " 'absolutely',\n",
       " 'delicious',\n",
       " 'definitely',\n",
       " 'worth',\n",
       " 'loaded',\n",
       " 'fries',\n",
       " 'biggest',\n",
       " 'letdown',\n",
       " 'ever',\n",
       " 'sad',\n",
       " 'soggy',\n",
       " 'affair',\n",
       " 'thin',\n",
       " 'dry',\n",
       " 'actually',\n",
       " 'funny',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'everyone',\n",
       " 'adores',\n",
       " 'them',\n",
       " \"wasn't\",\n",
       " 'extremely excited',\n",
       " 'excited go',\n",
       " 'go lunch',\n",
       " 'lunch at',\n",
       " 'at au',\n",
       " 'au cheval',\n",
       " 'cheval as',\n",
       " 'as soon',\n",
       " 'soon as',\n",
       " 'as indoor',\n",
       " 'indoor dining',\n",
       " 'dining approved',\n",
       " 'approved know',\n",
       " 'know have',\n",
       " 'have what',\n",
       " 'what considered',\n",
       " 'considered one',\n",
       " 'one best',\n",
       " 'best burgers',\n",
       " 'burgers all',\n",
       " 'all time',\n",
       " \"time couldn't\",\n",
       " \"couldn't wait\",\n",
       " 'wait try',\n",
       " 'try arrived',\n",
       " 'arrived very',\n",
       " 'very on',\n",
       " 'on top',\n",
       " 'top hygiene',\n",
       " 'hygiene took',\n",
       " 'took all',\n",
       " 'all precautions',\n",
       " 'precautions are',\n",
       " 'are led',\n",
       " 'led our',\n",
       " 'our seat',\n",
       " 'seat dining',\n",
       " 'dining room',\n",
       " 'room frigid',\n",
       " 'frigid assured',\n",
       " 'assured would',\n",
       " 'would warm',\n",
       " 'warm up',\n",
       " 'up sadly',\n",
       " 'sadly never',\n",
       " 'never did',\n",
       " 'did sat',\n",
       " 'sat looked',\n",
       " 'looked at',\n",
       " 'at menu',\n",
       " 'menu very',\n",
       " 'very pressed',\n",
       " 'pressed put',\n",
       " 'put our',\n",
       " 'our entire',\n",
       " 'entire order',\n",
       " 'order understand',\n",
       " 'understand want',\n",
       " 'want time',\n",
       " 'time out',\n",
       " 'out us',\n",
       " 'us quite',\n",
       " 'quite frankly',\n",
       " 'frankly felt',\n",
       " 'felt very',\n",
       " 'very rushed',\n",
       " 'rushed pushy',\n",
       " 'pushy started',\n",
       " 'started bloody',\n",
       " 'bloody mary',\n",
       " 'mary sadly',\n",
       " 'sadly good',\n",
       " 'good at',\n",
       " 'at all',\n",
       " 'all very',\n",
       " 'very bitter',\n",
       " 'bitter some',\n",
       " 'some bone',\n",
       " 'bone marrow',\n",
       " 'marrow very',\n",
       " 'very tasty',\n",
       " 'tasty then',\n",
       " 'then came',\n",
       " 'came caesar',\n",
       " 'caesar salad',\n",
       " 'salad absolutely',\n",
       " 'absolutely delicious',\n",
       " 'delicious definitely',\n",
       " 'definitely worth',\n",
       " 'worth try',\n",
       " 'try then',\n",
       " 'then our',\n",
       " 'our burgers',\n",
       " 'burgers loaded',\n",
       " 'loaded fries',\n",
       " 'fries biggest',\n",
       " 'biggest letdown',\n",
       " 'letdown ever',\n",
       " 'ever fries',\n",
       " 'fries sad',\n",
       " 'sad soggy',\n",
       " 'soggy affair',\n",
       " 'affair burgers',\n",
       " 'burgers so',\n",
       " 'so thin',\n",
       " 'thin dry',\n",
       " 'dry actually',\n",
       " 'actually funny',\n",
       " 'funny wish',\n",
       " 'wish could',\n",
       " 'could experience',\n",
       " 'experience what',\n",
       " 'what everyone',\n",
       " 'everyone adores',\n",
       " 'adores them',\n",
       " 'them sadly',\n",
       " \"sadly wasn't\",\n",
       " 'stopped',\n",
       " 'by',\n",
       " 'buy',\n",
       " 'couple',\n",
       " 'cupcakes',\n",
       " 'today',\n",
       " 'friend',\n",
       " 'always',\n",
       " 'love',\n",
       " \"molly's\",\n",
       " 'those',\n",
       " 'soft',\n",
       " 'moisture',\n",
       " 'hard',\n",
       " 'outside',\n",
       " 'cupcake',\n",
       " 'cookie',\n",
       " \"it's\",\n",
       " 'feels',\n",
       " 'has',\n",
       " 'sitting',\n",
       " 'shelf',\n",
       " 'overnight',\n",
       " 'least',\n",
       " 'days',\n",
       " 'disappointed',\n",
       " 'location',\n",
       " 'business',\n",
       " 'having',\n",
       " 'now',\n",
       " 'justify',\n",
       " 'serving',\n",
       " 'your',\n",
       " 'customers',\n",
       " 'stopped by',\n",
       " 'by at',\n",
       " 'at store',\n",
       " 'store buy',\n",
       " 'buy couple',\n",
       " 'couple cupcakes',\n",
       " 'cupcakes today',\n",
       " 'today friend',\n",
       " 'friend always',\n",
       " 'always love',\n",
       " \"love molly's\",\n",
       " \"molly's cupcakes\",\n",
       " 'cupcakes because',\n",
       " 'because those',\n",
       " 'those so',\n",
       " 'so tasty',\n",
       " 'tasty soft',\n",
       " 'soft moisture',\n",
       " 'moisture cupcakes',\n",
       " 'cupcakes got',\n",
       " 'got today',\n",
       " 'today very',\n",
       " 'very hard',\n",
       " 'hard on',\n",
       " 'on outside',\n",
       " 'outside cupcake',\n",
       " 'cupcake tasted',\n",
       " 'tasted like',\n",
       " 'like cookie',\n",
       " \"cookie it's\",\n",
       " \"it's feels\",\n",
       " 'feels cupcake',\n",
       " 'cupcake has',\n",
       " 'has been',\n",
       " 'been sitting',\n",
       " 'sitting on',\n",
       " 'on shelf',\n",
       " 'shelf overnight',\n",
       " 'overnight or',\n",
       " 'or at',\n",
       " 'at least',\n",
       " 'least couple',\n",
       " 'couple days',\n",
       " \"days i'm\",\n",
       " \"i'm very\",\n",
       " 'very disappointed',\n",
       " \"disappointed molly's\",\n",
       " \"molly's cupcake\",\n",
       " 'cupcake at',\n",
       " 'at location',\n",
       " 'location understand',\n",
       " 'understand business',\n",
       " 'business having',\n",
       " 'having hard',\n",
       " 'hard time',\n",
       " 'time right',\n",
       " 'right now',\n",
       " 'now still',\n",
       " \"still doesn't\",\n",
       " \"doesn't justify\",\n",
       " 'justify serving',\n",
       " 'serving hard',\n",
       " 'hard cupcakes',\n",
       " 'cupcakes your',\n",
       " 'your customers',\n",
       " 'forget',\n",
       " 'items',\n",
       " 'orders',\n",
       " \"i've\",\n",
       " 'here',\n",
       " 'row',\n",
       " 'each',\n",
       " 'day',\n",
       " 'hang',\n",
       " 'terrible customer',\n",
       " 'service always',\n",
       " 'always forget',\n",
       " 'forget items',\n",
       " 'items orders',\n",
       " \"orders i've\",\n",
       " \"i've been\",\n",
       " 'been here',\n",
       " 'here days',\n",
       " 'days row',\n",
       " 'row each',\n",
       " 'each day',\n",
       " 'day there',\n",
       " 'there always',\n",
       " 'always something',\n",
       " 'something missing',\n",
       " 'missing when',\n",
       " 'when you',\n",
       " 'you call',\n",
       " 'call just',\n",
       " 'just hang',\n",
       " 'hang up',\n",
       " 'up on',\n",
       " 'on you',\n",
       " 'phone',\n",
       " \"didn't\",\n",
       " 'get',\n",
       " 'woman',\n",
       " 'picks',\n",
       " 'reservation',\n",
       " 'guess',\n",
       " 'what rude',\n",
       " 'rude person',\n",
       " 'person on',\n",
       " 'on phone',\n",
       " \"phone didn't\",\n",
       " \"didn't even\",\n",
       " 'even get',\n",
       " 'get go',\n",
       " 'go there',\n",
       " 'there whoever',\n",
       " 'whoever woman',\n",
       " 'woman picks',\n",
       " 'picks up',\n",
       " 'up phone',\n",
       " 'phone make',\n",
       " 'make reservation',\n",
       " 'reservation very',\n",
       " 'unpleasant guess',\n",
       " 'guess will',\n",
       " 'will experience',\n",
       " 'experience place',\n",
       " 'begin',\n",
       " 'saying',\n",
       " 'unique',\n",
       " 'yelp',\n",
       " 'based',\n",
       " 'situation',\n",
       " 'presented',\n",
       " 'about',\n",
       " 'dots',\n",
       " 'dash',\n",
       " 'recently',\n",
       " 'check',\n",
       " 'amazing',\n",
       " 'little',\n",
       " 'tiki',\n",
       " 'bar',\n",
       " 'heart',\n",
       " 'downtown',\n",
       " 'various',\n",
       " 'limitations',\n",
       " 'covid',\n",
       " 'terms',\n",
       " 'capacity',\n",
       " 'yesterday',\n",
       " 'prior',\n",
       " 'leaving',\n",
       " 'home',\n",
       " 'currently',\n",
       " 'operating',\n",
       " 'employee',\n",
       " '\"yes',\n",
       " 'open.\"',\n",
       " 'reservations',\n",
       " 'needed',\n",
       " 'replied',\n",
       " '\"no',\n",
       " 'welcome',\n",
       " 'walk',\n",
       " 'in.\"',\n",
       " 'entrance',\n",
       " 'way',\n",
       " 'alley',\n",
       " 'soul',\n",
       " 'doors',\n",
       " 'locked',\n",
       " 'confirm',\n",
       " 'indeed',\n",
       " 'open',\n",
       " 'reassured',\n",
       " 'yes',\n",
       " 'front',\n",
       " 'building',\n",
       " 'proceed',\n",
       " 'signage',\n",
       " 'door',\n",
       " 'bub',\n",
       " 'city',\n",
       " 'asks',\n",
       " 'assures',\n",
       " 'temperatures',\n",
       " 'lets',\n",
       " 'table',\n",
       " 'inside',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 5.599422459331958,\n",
       " 'off putting': 6.725433722188183,\n",
       " 'rib pasta': 4.8283137373023015,\n",
       " 'divine main': 7.600902459542082,\n",
       " 'yes yes': 7.418580902748128,\n",
       " 'requiring reservations': 8.517193191416238,\n",
       " 'various sauces': 6.3771270279199666,\n",
       " 'line be': 7.824046010856292,\n",
       " 'courtesy covid': 8.517193191416238,\n",
       " 'small as': 5.7763531674910364,\n",
       " 'dining limit': 7.1308988302963465,\n",
       " 'ready have': 9.210340371976184,\n",
       " 'other pastas': 8.111728083308073,\n",
       " 'salad which': 6.16581793425276,\n",
       " 'covid new': 8.517193191416238,\n",
       " 'perhaps': 4.866534950122499,\n",
       " 'on butter': 6.812445099177812,\n",
       " 'want say': 5.991464547107982,\n",
       " 'figured would': 8.517193191416238,\n",
       " 'late night': 4.667045589706179,\n",
       " 'omg so': 7.418580902748128,\n",
       " \"seminar lou's\": 6.812445099177812,\n",
       " 'waited over': 7.418580902748128,\n",
       " 'stop here': 8.111728083308073,\n",
       " 'she has': 6.074846156047033,\n",
       " 'have seen': 8.517193191416238,\n",
       " 'differentiate them': 4.853631545286591,\n",
       " 'character': 6.3771270279199666,\n",
       " \"simple don't\": 7.418580902748128,\n",
       " 'while think': 7.600902459542082,\n",
       " 'honestly kind': 6.812445099177812,\n",
       " 'traditional ice': 9.210340371976184,\n",
       " 'as treat': 7.264430222920869,\n",
       " 'take back': 7.1308988302963465,\n",
       " 'twin': 7.824046010856292,\n",
       " \"it's safe\": 4.382026634673881,\n",
       " 'tempura': 6.645391014514646,\n",
       " 'jumbo believe': 5.843044541989709,\n",
       " 'vulnerability many': 8.111728083308073,\n",
       " 'wings which': 5.991464547107982,\n",
       " 'got today': 5.06720564558465,\n",
       " 'behind next': 7.824046010856292,\n",
       " 'unwrapped everything': 6.907755278982137,\n",
       " 'so order': 5.843044541989709,\n",
       " 'as bonus': 6.3771270279199666,\n",
       " 'great move': 6.907755278982137,\n",
       " 'so often': 7.1308988302963465,\n",
       " 'cash final': 6.3771270279199666,\n",
       " 'time ordered': 6.812445099177812,\n",
       " 'dine sunday': 7.600902459542082,\n",
       " 'badly about': 6.812445099177812,\n",
       " 'online classic': 6.812445099177812,\n",
       " 'latte chai': 7.264430222920869,\n",
       " 'seating came': 6.812445099177812,\n",
       " 'delightful food': 5.683979847360021,\n",
       " 'took advantage': 8.111728083308073,\n",
       " 'since she': 7.600902459542082,\n",
       " 'moist bread': 6.437751649736401,\n",
       " 'establishment does': 7.600902459542082,\n",
       " 'burger both': 6.3771270279199666,\n",
       " 'clarity ice': 7.418580902748128,\n",
       " 'calamari light': 7.824046010856292,\n",
       " 'hey pizza': 6.907755278982137,\n",
       " 'food divine': 7.264430222920869,\n",
       " \"weren't scared\": 8.517193191416238,\n",
       " 'sauce served': 6.645391014514646,\n",
       " 'request next': 6.907755278982137,\n",
       " 'by alternating': 6.907755278982137,\n",
       " 'katsu': 9.210340371976184,\n",
       " 'dry snuck': 7.418580902748128,\n",
       " 'becomes bowl': 7.600902459542082,\n",
       " 'bean without': 8.111728083308073,\n",
       " 'bigger order': 8.517193191416238,\n",
       " \"don't do\": 6.502290170873972,\n",
       " 'massive swear': 6.437751649736401,\n",
       " 'vongole linguini': 7.600902459542082,\n",
       " 'like texture': 7.824046010856292,\n",
       " 'which love': 6.502290170873972,\n",
       " 'shoppes': 6.907755278982137,\n",
       " 'tonight disappointed': 5.914503505971854,\n",
       " 'so thoughtful': 9.210340371976184,\n",
       " 'waiter tony': 7.264430222920869,\n",
       " 'closer so': 7.824046010856292,\n",
       " 'into two': 7.013115794639964,\n",
       " 'phase': 7.824046010856292,\n",
       " 'into past': 8.517193191416238,\n",
       " 'down taste': 5.8781358618009785,\n",
       " 'off plate': 6.437751649736401,\n",
       " 'around your': 9.210340371976184,\n",
       " 'cherry tasted': 9.210340371976184,\n",
       " 'cheese hot': 5.952243833954701,\n",
       " 'doughnut': 7.824046010856292,\n",
       " 'reheated oven': 7.1308988302963465,\n",
       " 'passed': 5.278514739251857,\n",
       " 'finally trying': 6.645391014514646,\n",
       " 'look no': 6.2659013928097425,\n",
       " 'pizza crust': 4.092346559559427,\n",
       " 'microwave cold': 5.360192770266124,\n",
       " 'line asking': 8.111728083308073,\n",
       " 'labor day': 6.3771270279199666,\n",
       " \"side taboule...that's\": 9.210340371976184,\n",
       " 'mean truly': 9.210340371976184,\n",
       " 'undercooked eggs': 6.812445099177812,\n",
       " 'sauce literally': 7.824046010856292,\n",
       " 'getting check': 5.472670753692815,\n",
       " 'practiced': 6.812445099177812,\n",
       " 'ramen try': 6.2659013928097425,\n",
       " 'between seoul': 7.264430222920869,\n",
       " 'cavernous den': 7.824046010856292,\n",
       " 'thank them': 6.571283042360924,\n",
       " 'good shape': 8.517193191416238,\n",
       " 'deep': 2.159350924908138,\n",
       " 'whiskey\" originally': 7.824046010856292,\n",
       " 'bus-boys our': 7.418580902748128,\n",
       " \"system couldn't\": 6.725433722188183,\n",
       " '\"give': 7.824046010856292,\n",
       " 'good takeout': 4.06284589516273,\n",
       " 'dimensional otherwise': 8.111728083308073,\n",
       " 'track': 7.1308988302963465,\n",
       " 'downtown will': 7.418580902748128,\n",
       " 'delivered food': 8.517193191416238,\n",
       " 'do book': 7.264430222920869,\n",
       " 'busy friday': 7.418580902748128,\n",
       " 'you so': 6.319968614080018,\n",
       " 'had individual': 8.111728083308073,\n",
       " 'down super': 6.3771270279199666,\n",
       " 'friend loves': 7.1308988302963465,\n",
       " \"probably won't\": 6.2659013928097425,\n",
       " 'hummus etc': 7.824046010856292,\n",
       " 'signature pasta': 9.210340371976184,\n",
       " 'since go': 7.1308988302963465,\n",
       " 'house run': 7.1308988302963465,\n",
       " 'tonight decided': 9.210340371976184,\n",
       " 'ordered cauliflower': 8.517193191416238,\n",
       " 'typically fan': 6.3771270279199666,\n",
       " 'can mess': 9.210340371976184,\n",
       " 'location ice': 7.418580902748128,\n",
       " 'pm from': 7.824046010856292,\n",
       " 'likes humming': 8.111728083308073,\n",
       " 'nachos have': 6.907755278982137,\n",
       " 'website breeze': 7.264430222920869,\n",
       " 'leftovers two': 6.571283042360924,\n",
       " 'nice thing': 8.517193191416238,\n",
       " 'half mile': 6.3771270279199666,\n",
       " 'types sauces': 7.013115794639964,\n",
       " 'cheese yaaaaas': 9.210340371976184,\n",
       " 'server attentive': 7.1308988302963465,\n",
       " 'absolutely nothing': 7.600902459542082,\n",
       " 'one most': 4.803621124711929,\n",
       " 'norm waited': 7.1308988302963465,\n",
       " 'say their': 6.3771270279199666,\n",
       " 'different appetizer': 7.600902459542082,\n",
       " 'went sweet': 9.210340371976184,\n",
       " 'made even': 7.824046010856292,\n",
       " 'list places': 6.907755278982137,\n",
       " 'photogenic': 8.517193191416238,\n",
       " 'calling front': 7.013115794639964,\n",
       " 'tequila': 7.418580902748128,\n",
       " 'left round': 5.809142990314028,\n",
       " 'fine his': 7.824046010856292,\n",
       " 'included some': 8.517193191416238,\n",
       " 'arrived within': 9.210340371976184,\n",
       " \"he wasn't\": 6.907755278982137,\n",
       " 'up put': 4.699480865459333,\n",
       " 'fries might': 6.502290170873972,\n",
       " 'families': 7.013115794639964,\n",
       " 'perfectly had': 7.1308988302963465,\n",
       " 'craft': 6.645391014514646,\n",
       " 'window able': 7.264430222920869,\n",
       " 'well topped': 7.824046010856292,\n",
       " 'tolerant all': 7.600902459542082,\n",
       " 'off probably': 4.721704002244043,\n",
       " 'made diced': 8.111728083308073,\n",
       " 'wait allow': 5.914503505971854,\n",
       " 'herb': 6.812445099177812,\n",
       " 'other diners': 6.812445099177812,\n",
       " 'back house': 4.06284589516273,\n",
       " 'touch sides': 9.210340371976184,\n",
       " 'towards your': 5.298317366548036,\n",
       " 'top tables': 6.907755278982137,\n",
       " 'often though': 5.472670753692815,\n",
       " 'roll': 4.8283137373023015,\n",
       " 'enjoyed strawberry': 7.1308988302963465,\n",
       " 'apos;dine-in-tent&apos experience': 7.600902459542082,\n",
       " 'run its': 7.1308988302963465,\n",
       " 'sweet fat': 5.744604469176457,\n",
       " 'when asked': 4.688551794927142,\n",
       " \"mushy didn't\": 4.439655747510518,\n",
       " 'some sweetness': 7.013115794639964,\n",
       " 'bottom ours': 6.16581793425276,\n",
       " 'nice date': 6.907755278982137,\n",
       " 'exceptional food': 9.210340371976184,\n",
       " 'first retail': 8.111728083308073,\n",
       " 'different custom': 6.812445099177812,\n",
       " 'spiciness': 6.437751649736401,\n",
       " 'test household': 7.1308988302963465,\n",
       " 'hear': 5.952243833954701,\n",
       " 'before did': 7.824046010856292,\n",
       " 'service fees': 7.1308988302963465,\n",
       " 'hyped made': 6.437751649736401,\n",
       " 'tomato aioli': 6.907755278982137,\n",
       " 'hearty tastes': 9.210340371976184,\n",
       " \"also wasn't\": 5.403677882205863,\n",
       " 'seating front': 5.952243833954701,\n",
       " \"awhile don't\": 5.8781358618009785,\n",
       " 'astounded by': 5.683979847360021,\n",
       " 'fusion': 5.360192770266124,\n",
       " 'recommended from': 7.1308988302963465,\n",
       " 'dark here': 6.645391014514646,\n",
       " 'many folks': 8.517193191416238,\n",
       " 'potential contamination': 6.812445099177812,\n",
       " 'extremely well': 6.725433722188183,\n",
       " 'evanston': 7.600902459542082,\n",
       " 'deck bbq': 7.013115794639964,\n",
       " 'fall think': 6.645391014514646,\n",
       " 'peak times': 7.600902459542082,\n",
       " 'solo vaca': 8.111728083308073,\n",
       " 'alleyway which': 4.866534950122499,\n",
       " 'welcomed': 5.149897361429764,\n",
       " 'down block': 6.3771270279199666,\n",
       " 'dolo': 5.914503505971854,\n",
       " 'perfect blend': 7.013115794639964,\n",
       " 'left review': 8.517193191416238,\n",
       " 'share': 4.226733750267846,\n",
       " 'better dipping': 5.952243833954701,\n",
       " 'remembered our': 7.600902459542082,\n",
       " 'menu options': 6.571283042360924,\n",
       " 'recognized as': 8.111728083308073,\n",
       " 'cry': 7.600902459542082,\n",
       " 'place could': 8.517193191416238,\n",
       " 'mins an': 5.06720564558465,\n",
       " 'floor': 5.914503505971854,\n",
       " 'sometimes take': 7.1308988302963465,\n",
       " 'silver': 5.914503505971854,\n",
       " 'attentive all': 9.210340371976184,\n",
       " 'end girlfriend': 7.600902459542082,\n",
       " 'stars have': 7.418580902748128,\n",
       " 'good \\u200d': 6.812445099177812,\n",
       " 'delicious has': 7.824046010856292,\n",
       " 'however due': 7.264430222920869,\n",
       " 'very provocative': 6.907755278982137,\n",
       " 'degrees minutes': 7.1308988302963465,\n",
       " 'cool bar': 6.812445099177812,\n",
       " \"you're using\": 7.1308988302963465,\n",
       " 'between okay': 6.907755278982137,\n",
       " 'pizza very': 4.034190639402354,\n",
       " 'timers ambience': 5.914503505971854,\n",
       " 'plenty warm': 7.1308988302963465,\n",
       " 'messy': 4.537511537514277,\n",
       " 'stuffed medjool': 8.111728083308073,\n",
       " 'enough service': 6.2659013928097425,\n",
       " 'penny service': 7.418580902748128,\n",
       " 'do about': 6.319968614080018,\n",
       " 'great ramen': 8.517193191416238,\n",
       " '+ are': 9.210340371976184,\n",
       " 'yourself makes': 9.210340371976184,\n",
       " 'days row': 4.350527967614511,\n",
       " 'high-backed chairs': 6.812445099177812,\n",
       " 'competition bbq': 6.214608098422191,\n",
       " 'items ever': 6.812445099177812,\n",
       " 'tender top': 7.1308988302963465,\n",
       " 'over time': 6.645391014514646,\n",
       " 'be popping': 6.3771270279199666,\n",
       " \"he's so\": 7.824046010856292,\n",
       " 'order chinese': 7.264430222920869,\n",
       " 'jambalaya omelette': 7.824046010856292,\n",
       " 'brings': 5.572754212249797,\n",
       " 'guests menu': 8.517193191416238,\n",
       " 'reduces amount': 6.645391014514646,\n",
       " 'today friend': 5.06720564558465,\n",
       " 'unique had': 8.111728083308073,\n",
       " 'best then': 7.418580902748128,\n",
       " 'share are': 7.264430222920869,\n",
       " 'options which': 8.517193191416238,\n",
       " 'rest': 3.6045383056801854,\n",
       " 'viruses': 6.074846156047033,\n",
       " 'which have': 6.812445099177812,\n",
       " 'bread someone': 4.12893600699172,\n",
       " 'cones expected': 8.111728083308073,\n",
       " 'said close': 5.221356325411908,\n",
       " 'turning tables': 5.381698975487088,\n",
       " 'greasy amazingly': 7.264430222920869,\n",
       " 'which favorite': 7.264430222920869,\n",
       " 'took half': 9.210340371976184,\n",
       " 'balances': 7.1308988302963465,\n",
       " 'from these': 9.210340371976184,\n",
       " 'brusque': 7.824046010856292,\n",
       " 'past chicago': 4.154094566627875,\n",
       " 'be horrendous': 8.517193191416238,\n",
       " 'certainly deserves': 7.418580902748128,\n",
       " 'bbq can': 7.264430222920869,\n",
       " 'put': 2.1311559773665145,\n",
       " 'moving': 3.825845309187094,\n",
       " 'force you': 9.210340371976184,\n",
       " 'pleasure walk': 7.600902459542082,\n",
       " 'spot \"what': 7.824046010856292,\n",
       " 'cheval burger': 5.8781358618009785,\n",
       " 'indian': 6.119297918617867,\n",
       " 'mix you': 7.1308988302963465,\n",
       " 'scaled high': 8.111728083308073,\n",
       " 'match just': 6.645391014514646,\n",
       " 'path lovely': 7.600902459542082,\n",
       " 'or videos': 9.210340371976184,\n",
       " 'away our': 6.074846156047033,\n",
       " 'horses': 6.3771270279199666,\n",
       " 'fine if': 6.3771270279199666,\n",
       " 'need cheese': 7.824046010856292,\n",
       " 'delicious give': 8.517193191416238,\n",
       " 'batter amp': 7.264430222920869,\n",
       " 'resolved': 6.725433722188183,\n",
       " 'option satisfied': 4.721704002244043,\n",
       " 'captain america': 6.725433722188183,\n",
       " 'son really': 6.645391014514646,\n",
       " 'rice either': 6.725433722188183,\n",
       " 'ny': 5.240048458424061,\n",
       " 'side sausage': 5.426150738057921,\n",
       " \"use it's\": 5.8781358618009785,\n",
       " 'them being': 6.214608098422191,\n",
       " 'vegetarian dishes': 8.111728083308073,\n",
       " 'burger first': 6.645391014514646,\n",
       " 'go you': 6.812445099177812,\n",
       " 'out really': 8.517193191416238,\n",
       " 'cupcakes your': 5.06720564558465,\n",
       " 'creamy dislike': 6.907755278982137,\n",
       " 'like combination': 7.013115794639964,\n",
       " 'reliable': 7.600902459542082,\n",
       " 'around area': 7.418580902748128,\n",
       " 'bringing your': 7.013115794639964,\n",
       " 'food justifies': 6.3771270279199666,\n",
       " 'deninitely': 7.013115794639964,\n",
       " 'tried pulled': 7.600902459542082,\n",
       " \"nearby wouldn't\": 7.264430222920869,\n",
       " 'almost immediately': 5.572754212249797,\n",
       " 'our recent': 7.600902459542082,\n",
       " 'usual girlfriend': 7.418580902748128,\n",
       " 'ny strip': 9.210340371976184,\n",
       " \"doesn't catch\": 7.418580902748128,\n",
       " 'fillings you': 6.319968614080018,\n",
       " 'more favors': 7.264430222920869,\n",
       " 'bummer': 6.214608098422191,\n",
       " 'fancy kentucky': 6.3771270279199666,\n",
       " 'offer other': 7.264430222920869,\n",
       " 'nice be': 7.264430222920869,\n",
       " 'an annoying': 9.210340371976184,\n",
       " 'shirt as': 4.779523573132869,\n",
       " 'hearty oh': 8.111728083308073,\n",
       " 'will check': 6.725433722188183,\n",
       " 'ravioli squid': 5.599422459331958,\n",
       " 'are crust': 4.154094566627875,\n",
       " \"enjoyed didn't\": 7.824046010856292,\n",
       " 'kids ordered': 6.645391014514646,\n",
       " 'into serious': 9.210340371976184,\n",
       " 'reviews based': 4.961845129926823,\n",
       " 'booth when': 6.645391014514646,\n",
       " 'eaten here': 9.210340371976184,\n",
       " 'cheap cupcakes': 7.264430222920869,\n",
       " 'his best': 5.546778725846536,\n",
       " 'pouch sealing': 7.824046010856292,\n",
       " 'sounded too': 6.725433722188183,\n",
       " 'eyes pick': 7.1308988302963465,\n",
       " 'minneapolis order': 6.812445099177812,\n",
       " 'huge flavorful': 7.824046010856292,\n",
       " \"it's sort\": 6.907755278982137,\n",
       " 'has rough': 6.074846156047033,\n",
       " 'toast which': 4.9062752787720125,\n",
       " 'omelettes all': 8.111728083308073,\n",
       " 'sausage big': 7.824046010856292,\n",
       " 'talked': 6.725433722188183,\n",
       " 'which basically': 4.815891217303744,\n",
       " 'great value': 6.812445099177812,\n",
       " 'pass': 4.509860006183766,\n",
       " 'hot also': 6.812445099177812,\n",
       " 'about so': 5.472670753692815,\n",
       " 'like just': 5.546778725846536,\n",
       " 'remember names': 7.600902459542082,\n",
       " 'bay': 6.571283042360924,\n",
       " 'midnight when': 5.221356325411908,\n",
       " 'dish venue': 7.264430222920869,\n",
       " 'order lady': 8.517193191416238,\n",
       " 'dishes try': 7.600902459542082,\n",
       " 'watched tracker': 6.2659013928097425,\n",
       " 'yelp app': 9.210340371976184,\n",
       " 'delicious well-cooked': 7.264430222920869,\n",
       " 'cheap ideal': 6.3771270279199666,\n",
       " 'order delivery': 5.496768305271875,\n",
       " 'lobby': 4.744432253321599,\n",
       " 'liked short': 7.600902459542082,\n",
       " 'salad great': 6.907755278982137,\n",
       " 'higher priced': 8.111728083308073,\n",
       " 'sushi salads': 8.111728083308073,\n",
       " 'strips various': 6.3771270279199666,\n",
       " 'piece holds': 6.907755278982137,\n",
       " 'pizza gardinera': 9.210340371976184,\n",
       " 'rotisserie': 7.418580902748128,\n",
       " 'anything wished': 7.418580902748128,\n",
       " 'extra bread': 7.1308988302963465,\n",
       " 'thought day': 4.8283137373023015,\n",
       " 'made bourbon': 8.517193191416238,\n",
       " 'here bongo': 8.517193191416238,\n",
       " 'signature fudge': 8.517193191416238,\n",
       " 'mantou bun': 8.517193191416238,\n",
       " 'spinach mushroom': 5.7763531674910364,\n",
       " 'mask car': 9.210340371976184,\n",
       " 'had turkish': 9.210340371976184,\n",
       " 'parcel': 7.264430222920869,\n",
       " 'like have': 4.56594947283481,\n",
       " 'aquarium smokers': 6.214608098422191,\n",
       " 'where ordered': 5.952243833954701,\n",
       " 'are like': 7.013115794639964,\n",
       " 'amazing cranberries': 8.111728083308073,\n",
       " 'so waiting': 7.824046010856292,\n",
       " 'sense': 5.403677882205863,\n",
       " 'lit tree': 7.600902459542082,\n",
       " 'crunchy on': 8.111728083308073,\n",
       " 'offered his': 9.210340371976184,\n",
       " 'so tip': 6.645391014514646,\n",
       " 'without stepping': 6.571283042360924,\n",
       " 'ourselves': 5.240048458424061,\n",
       " 'few customers': 9.210340371976184,\n",
       " 'seated immediate': 7.600902459542082,\n",
       " 'uber driver': 5.546778725846536,\n",
       " 'felt better': 7.418580902748128,\n",
       " \"jane's honey-fried\": 6.907755278982137,\n",
       " 'morning being': 6.645391014514646,\n",
       " 'expensive amp': 6.214608098422191,\n",
       " 'impressive think': 7.264430222920869,\n",
       " 'creamy too': 7.600902459542082,\n",
       " 'going eat': 7.1308988302963465,\n",
       " 'pepperoni pizza': 4.275866438845491,\n",
       " 'covid being': 5.426150738057921,\n",
       " 'good golly': 7.824046010856292,\n",
       " 'give crisp': 7.418580902748128,\n",
       " 'their korean': 6.214608098422191,\n",
       " 'octopus': 5.44914025628262,\n",
       " 'bathroom suspect': 6.032286541628237,\n",
       " 'are finishing': 4.06284589516273,\n",
       " 'hit more': 6.645391014514646,\n",
       " 'order took': 7.1308988302963465,\n",
       " 'reservations parking': 7.824046010856292,\n",
       " 'service act': 4.744432253321599,\n",
       " \"friend can't\": 4.439655747510518,\n",
       " 'know when': 4.9062752787720125,\n",
       " 'rarely checked-in': 5.599422459331958,\n",
       " 'exceptional': 5.546778725846536,\n",
       " 'been handwritten': 7.264430222920869,\n",
       " 'side loved': 7.824046010856292,\n",
       " 'casual dining': 7.013115794639964,\n",
       " 'around way': 8.111728083308073,\n",
       " 'beautiful sight': 8.517193191416238,\n",
       " 'together,but its': 7.600902459542082,\n",
       " 'highlight': 5.2030071867437115,\n",
       " 'covid regulations': 4.431216878864653,\n",
       " 'since march': 5.149897361429764,\n",
       " 'breakfast some': 7.264430222920869,\n",
       " 'ranch fantastic': 6.645391014514646,\n",
       " 'late november': 7.600902459542082,\n",
       " 'definitely dress': 8.111728083308073,\n",
       " 'known options': 7.600902459542082,\n",
       " 'real soon': 6.571283042360924,\n",
       " 'mediocre ramen': 6.2659013928097425,\n",
       " 'fierce': 6.214608098422191,\n",
       " 'longer our': 4.9062752787720125,\n",
       " 'night city': 7.824046010856292,\n",
       " 'rather choose': 7.600902459542082,\n",
       " 'about cocktails': 7.600902459542082,\n",
       " 'as stuffed': 7.824046010856292,\n",
       " 'better places': 6.032286541628237,\n",
       " 'hoping next': 9.210340371976184,\n",
       " 'perfect mix': 7.600902459542082,\n",
       " 'offer has': 8.517193191416238,\n",
       " 'definitely have': 8.517193191416238,\n",
       " 'peppers amp': 9.210340371976184,\n",
       " 'egg nice': 6.812445099177812,\n",
       " 'other major': 6.645391014514646,\n",
       " 'cheese melts': 6.725433722188183,\n",
       " 'cove': 8.111728083308073,\n",
       " 'galore': 9.210340371976184,\n",
       " 'mocktail': 6.437751649736401,\n",
       " 'try pastas': 9.210340371976184,\n",
       " 'atmosphere nice': 7.1308988302963465,\n",
       " 'pizza places': 4.398156016603765,\n",
       " 'bunch outdoor': 9.210340371976184,\n",
       " 'legs stay': 9.210340371976184,\n",
       " 'to-go experience': 8.111728083308073,\n",
       " 'on out': 7.824046010856292,\n",
       " 'try even': 9.210340371976184,\n",
       " 'perfect temp': 8.111728083308073,\n",
       " 'eating au': 6.645391014514646,\n",
       " 'gelato uni': 8.517193191416238,\n",
       " 'honking': 7.600902459542082,\n",
       " 'gwen': 6.3771270279199666,\n",
       " 'someday would': 6.725433722188183,\n",
       " 'confit spring': 7.600902459542082,\n",
       " 'up w': 7.418580902748128,\n",
       " 'slightly wet': 7.824046010856292,\n",
       " 'inducing me': 7.418580902748128,\n",
       " 'buzz all': 8.517193191416238,\n",
       " 'come whatever': 5.149897361429764,\n",
       " 'tomato sauce': 5.403677882205863,\n",
       " 'kitchen closed': 7.1308988302963465,\n",
       " 'panna': 6.119297918617867,\n",
       " 'myself bit': 8.517193191416238,\n",
       " 'covid guidance': 8.517193191416238,\n",
       " 'takeouts pandemic': 8.111728083308073,\n",
       " 'report on': 7.013115794639964,\n",
       " \"means it's\": 7.418580902748128,\n",
       " 'husband likes': 6.907755278982137,\n",
       " 'telling me': 4.092346559559427,\n",
       " 'favorite tapas': 6.571283042360924,\n",
       " 'customer would': 6.812445099177812,\n",
       " 'when worry': 7.1308988302963465,\n",
       " 'wanna be': 4.439655747510518,\n",
       " 'location as': 7.264430222920869,\n",
       " 'looking delicious': 7.824046010856292,\n",
       " 'you realize': 5.683979847360021,\n",
       " 'danish': 8.517193191416238,\n",
       " 'stations': 9.210340371976184,\n",
       " 'dose umami': 7.600902459542082,\n",
       " 'birthday cake': 6.812445099177812,\n",
       " 'sassy half': 7.418580902748128,\n",
       " 'area starving': 5.472670753692815,\n",
       " 'dead': 7.264430222920869,\n",
       " \"i'm pictorial\": 8.111728083308073,\n",
       " 'birthday celebration': 7.1308988302963465,\n",
       " 'brought water': 9.210340371976184,\n",
       " 'bread salad': 7.418580902748128,\n",
       " 'did love': 5.051457288616511,\n",
       " 'heaping amount': 4.866534950122499,\n",
       " 'roll roll': 7.1308988302963465,\n",
       " 'perfectly fried': 5.683979847360021,\n",
       " 'split into': 7.013115794639964,\n",
       " 'drinks are': 4.755993075722675,\n",
       " 'f**k what': 6.812445099177812,\n",
       " 'heated lamps': 9.210340371976184,\n",
       " 'latest during': 8.111728083308073,\n",
       " 'get small': 7.013115794639964,\n",
       " 'diabetic': 8.517193191416238,\n",
       " 'eat there': 5.683979847360021,\n",
       " 'favorite pre-covid': 6.2659013928097425,\n",
       " 'toast tastes': 4.853631545286591,\n",
       " 'comes pizza': 5.713832810509702,\n",
       " 'relatively': 4.528209144851963,\n",
       " 'parked there': 8.517193191416238,\n",
       " 'fries nothing': 6.571283042360924,\n",
       " 'joy courage': 8.111728083308073,\n",
       " 'reheated unsure': 9.210340371976184,\n",
       " 'legendary as': 6.645391014514646,\n",
       " '\"cheese crusted': 7.264430222920869,\n",
       " 'favorite spot': 7.418580902748128,\n",
       " 'incredibly soft': 5.952243833954701,\n",
       " 'paella more': 7.013115794639964,\n",
       " 'scoop coffee': 8.111728083308073,\n",
       " 'chinese': 6.319968614080018,\n",
       " 'bill add': 5.472670753692815,\n",
       " 'strongly': 6.214608098422191,\n",
       " 'cream ice': 7.600902459542082,\n",
       " 'ahold': 8.111728083308073,\n",
       " 'like hawaii': 7.013115794639964,\n",
       " 'shoots': 9.210340371976184,\n",
       " 'uber when': 6.2659013928097425,\n",
       " 'savory slightly': 7.013115794639964,\n",
       " 'order as': 7.824046010856292,\n",
       " 'cheese its': 9.210340371976184,\n",
       " 'meals are': 7.600902459542082,\n",
       " 'dash few': 7.1308988302963465,\n",
       " 'spot filled': 9.210340371976184,\n",
       " 'these so': 7.600902459542082,\n",
       " 'recommend those': 8.111728083308073,\n",
       " 'drinks ice': 9.210340371976184,\n",
       " 'oysters cheese': 6.3771270279199666,\n",
       " 'place looked': 8.517193191416238,\n",
       " 'pickup cashless': 7.013115794639964,\n",
       " 'some': 1.5999827536633453,\n",
       " 'both decided': 7.264430222920869,\n",
       " 'walk around': 7.824046010856292,\n",
       " \"it's thin\": 7.418580902748128,\n",
       " 'ahogada torta': 5.403677882205863,\n",
       " 'new have': 9.210340371976184,\n",
       " 'wait be': 7.1308988302963465,\n",
       " 'telling you': 7.013115794639964,\n",
       " 'alluring': 8.517193191416238,\n",
       " 'think made': 7.824046010856292,\n",
       " 'pandemic situation': 7.600902459542082,\n",
       " 'packed great': 7.1308988302963465,\n",
       " 'left she': 5.149897361429764,\n",
       " 'screaming': 6.812445099177812,\n",
       " 'brought some': 7.013115794639964,\n",
       " 'out world': 5.626821433520073,\n",
       " 'happened': 3.733876820044672,\n",
       " 'area offered': 7.824046010856292,\n",
       " 'overwhelming staff': 7.600902459542082,\n",
       " 'mustard those': 5.914503505971854,\n",
       " 'what works': 7.013115794639964,\n",
       " 'pizzas making': 6.645391014514646,\n",
       " \"who'd never\": 4.721704002244043,\n",
       " 'get toast': 6.725433722188183,\n",
       " 'their dishes': 8.111728083308073,\n",
       " 'strollers few': 6.812445099177812,\n",
       " \"i'm fan\": 7.418580902748128,\n",
       " 'here finally': 4.853631545286591,\n",
       " \"you're transported\": 7.013115794639964,\n",
       " 'went ready': 9.210340371976184,\n",
       " 'naked': 6.907755278982137,\n",
       " 'truffle delicious': 7.824046010856292,\n",
       " 'smelling like': 6.437751649736401,\n",
       " 'belgian waffle': 7.013115794639964,\n",
       " 'so wait': 5.381698975487088,\n",
       " 'are thanks': 7.600902459542082,\n",
       " \"there's sanitizer\": 9.210340371976184,\n",
       " 'r hotels': 6.502290170873972,\n",
       " 'had garlic': 5.843044541989709,\n",
       " \"don't start\": 4.699480865459333,\n",
       " 'so there': 4.919880930827792,\n",
       " 'pretty busy': 6.502290170873972,\n",
       " 'by boxes': 9.210340371976184,\n",
       " 'order food': 6.2659013928097425,\n",
       " 'got fork': 7.418580902748128,\n",
       " 'skillet time': 7.600902459542082,\n",
       " 'now are': 5.360192770266124,\n",
       " 'bar top': 7.824046010856292,\n",
       " \"good wasn't\": 6.3771270279199666,\n",
       " 'good pork': 7.418580902748128,\n",
       " 'squeamish like': 7.600902459542082,\n",
       " 'about would': 6.032286541628237,\n",
       " 'there arent': 5.952243833954701,\n",
       " 'satay peanut': 7.264430222920869,\n",
       " 'any chicago': 7.600902459542082,\n",
       " 'apply too': 6.214608098422191,\n",
       " 'other conference': 8.111728083308073,\n",
       " 'half seoul': 7.418580902748128,\n",
       " 'overall pizza': 7.824046010856292,\n",
       " 'get strong': 7.1308988302963465,\n",
       " 'had blt': 7.824046010856292,\n",
       " 'out covid': 5.099466507802871,\n",
       " 'stone plates': 8.111728083308073,\n",
       " 'melt-in-mouth': 9.210340371976184,\n",
       " 'beyond expectation': 7.418580902748128,\n",
       " 'literally cannot': 8.111728083308073,\n",
       " 'heard lines': 6.645391014514646,\n",
       " 'thing could': 6.645391014514646,\n",
       " 'bottle your': 7.824046010856292,\n",
       " 'mediterranean': 4.656463480375642,\n",
       " 'some kfc': 6.3771270279199666,\n",
       " 'happy addition': 8.517193191416238,\n",
       " 'guaranteed': 6.725433722188183,\n",
       " 'our expectations': 6.032286541628237,\n",
       " 'broadway': 8.111728083308073,\n",
       " 'wait place': 7.824046010856292,\n",
       " 'required': 5.991464547107982,\n",
       " 'alley there': 4.961845129926823,\n",
       " 'sauce taken': 7.418580902748128,\n",
       " 'again however': 7.264430222920869,\n",
       " 'busy are': 5.278514739251857,\n",
       " 'display': 6.645391014514646,\n",
       " 'ginger dressing': 7.600902459542082,\n",
       " 'when making': 5.298317366548036,\n",
       " 'waitress very': 6.645391014514646,\n",
       " 'time want': 7.600902459542082,\n",
       " 'steak hotel': 7.264430222920869,\n",
       " 'seating cute': 5.149897361429764,\n",
       " 'if pre': 5.360192770266124,\n",
       " 'gigio popped': 9.210340371976184,\n",
       " 'literally worth': 6.812445099177812,\n",
       " 'creative food': 8.517193191416238,\n",
       " \"types backgrounds-it's\": 6.812445099177812,\n",
       " 'much so': 7.600902459542082,\n",
       " 'burgers at': 6.3771270279199666,\n",
       " 'employee very': 6.725433722188183,\n",
       " 'clean': 4.057048777478403,\n",
       " 'try time': 8.111728083308073,\n",
       " 'small diner': 6.812445099177812,\n",
       " 'anything would': 4.866534950122499,\n",
       " 'bigger proportion': 7.1308988302963465,\n",
       " 'too liked': 9.210340371976184,\n",
       " 'elements.\" you': 5.298317366548036,\n",
       " 'devoured them': 8.111728083308073,\n",
       " 'located off': 8.517193191416238,\n",
       " 'little bit': 4.919880930827792,\n",
       " 'given various': 9.210340371976184,\n",
       " 'me nervous': 5.952243833954701,\n",
       " 'hyde park': 7.1308988302963465,\n",
       " 'an entire': 7.418580902748128,\n",
       " 'vegetables creamy': 7.600902459542082,\n",
       " 'strawberry french': 7.418580902748128,\n",
       " 'cash as': 7.600902459542082,\n",
       " 'how food': 5.952243833954701,\n",
       " 'during frying': 5.7763531674910364,\n",
       " 'closing': 6.571283042360924,\n",
       " 'burger or': 7.013115794639964,\n",
       " 'have time': 5.381698975487088,\n",
       " 'menu caribbean': 8.517193191416238,\n",
       " 'on michigan': 7.264430222920869,\n",
       " 'formed before': 6.074846156047033,\n",
       " 'frozen rose': 9.210340371976184,\n",
       " 'sauces beware': 6.3771270279199666,\n",
       " 'loved menu': 8.517193191416238,\n",
       " 'staff food': 7.1308988302963465,\n",
       " 'tasty hot': 5.991464547107982,\n",
       " 'liked caramelized': 5.44914025628262,\n",
       " 'packaged so': 7.264430222920869,\n",
       " 'she recommended': 7.600902459542082,\n",
       " 'fellow yelpers': 7.264430222920869,\n",
       " 'here which': 8.517193191416238,\n",
       " 'steak amp': 9.210340371976184,\n",
       " 'blow me': 7.824046010856292,\n",
       " 'curve': 6.214608098422191,\n",
       " 'when scan': 4.961845129926823,\n",
       " 'air smiles': 7.824046010856292,\n",
       " 'has had': 9.210340371976184,\n",
       " 'saved': 6.725433722188183,\n",
       " 'few pieces': 4.710530701645918,\n",
       " 'attack on': 4.12893600699172,\n",
       " 'quite some': 7.264430222920869,\n",
       " 'night he': 8.111728083308073,\n",
       " 'theme amp': 7.418580902748128,\n",
       " 'visited often': 9.210340371976184,\n",
       " 'by loud': 7.013115794639964,\n",
       " 'light too': 8.111728083308073,\n",
       " 'party during': 8.111728083308073,\n",
       " 'shrimp you': 9.210340371976184,\n",
       " 'during crazy': 5.149897361429764,\n",
       " 'waiter broth': 8.111728083308073,\n",
       " \"tilted doesn't\": 7.824046010856292,\n",
       " 'like come': 8.111728083308073,\n",
       " 'one oldest': 8.517193191416238,\n",
       " 'precautions followed': 8.111728083308073,\n",
       " 'again because': 8.111728083308073,\n",
       " 'crispy fried': 7.418580902748128,\n",
       " 'which rotates': 7.824046010856292,\n",
       " 'shorted': 6.032286541628237,\n",
       " 'peach tart': 5.914503505971854,\n",
       " 'delicato': 8.517193191416238,\n",
       " 'here pre-covid': 5.914503505971854,\n",
       " 'awhile jump': 8.517193191416238,\n",
       " 'think patio': 7.264430222920869,\n",
       " 'well go': 6.3771270279199666,\n",
       " 'detergent': 6.074846156047033,\n",
       " 'compares': 8.111728083308073,\n",
       " 'tasteless bread': 5.7763531674910364,\n",
       " 'their insight': 5.683979847360021,\n",
       " 'difficult find': 8.517193191416238,\n",
       " 'memories': 8.517193191416238,\n",
       " 'felt well': 7.013115794639964,\n",
       " 'running out': 8.517193191416238,\n",
       " 'have couple': 8.111728083308073,\n",
       " 'first visited': 7.264430222920869,\n",
       " 'must-haves include': 8.517193191416238,\n",
       " 'before waffle': 9.210340371976184,\n",
       " 'umami': 7.600902459542082,\n",
       " 'happy gf': 6.725433722188183,\n",
       " 'deliver orders': 6.907755278982137,\n",
       " 'some people': 6.319968614080018,\n",
       " 'fresh plentiful': 8.111728083308073,\n",
       " 'hidden gem': 7.600902459542082,\n",
       " 'him went': 7.824046010856292,\n",
       " 'tonight very': 4.358310108056566,\n",
       " 'nooks': 6.812445099177812,\n",
       " 'socially distanced': 6.3771270279199666,\n",
       " 'elements together': 7.600902459542082,\n",
       " \"it's somewhat\": 6.645391014514646,\n",
       " 'same person': 7.600902459542082,\n",
       " 'delicious broth': 8.517193191416238,\n",
       " 'hour instead': 7.600902459542082,\n",
       " 'meatballs equally': 8.517193191416238,\n",
       " 'meal short': 6.812445099177812,\n",
       " 'advance picking': 6.725433722188183,\n",
       " 'manage expectations': 5.360192770266124,\n",
       " 'cause covid': 7.824046010856292,\n",
       " 'different kinds': 7.264430222920869,\n",
       " 'sauce salmon': 9.210340371976184,\n",
       " 'chicago trip': 9.210340371976184,\n",
       " 'recently heard': 8.517193191416238,\n",
       " 'friend hardly': 7.600902459542082,\n",
       " 'youre into': 6.502290170873972,\n",
       " 'one spot': 6.907755278982137,\n",
       " 'offering an': 7.824046010856292,\n",
       " 'making superb': 8.517193191416238,\n",
       " 'inside ambiance': 7.1308988302963465,\n",
       " 'apart split': 5.952243833954701,\n",
       " 'has all': 8.517193191416238,\n",
       " 'world if': 4.039856376938031,\n",
       " 'no disrespect': 4.06284589516273,\n",
       " 'shitcago deep': 4.154094566627875,\n",
       " 'place found': 6.812445099177812,\n",
       " 'reservation on': 7.264430222920869,\n",
       " \"rated i've\": 4.240527072400182,\n",
       " 'out delivery': 7.418580902748128,\n",
       " 'drinks if': 7.418580902748128,\n",
       " 'ramen....bruh ramen': 8.517193191416238,\n",
       " '\"good': 4.615220521841593,\n",
       " 'rare see': 7.824046010856292,\n",
       " 'seating so': 6.812445099177812,\n",
       " 'difficult times': 7.824046010856292,\n",
       " 'exactly like': 8.517193191416238,\n",
       " 'wind decided': 5.683979847360021,\n",
       " 'or pleasure': 8.517193191416238,\n",
       " 'or something': 4.892852258439873,\n",
       " 'chance participate': 7.824046010856292,\n",
       " 'new orleans': 6.437751649736401,\n",
       " 'pizza reminded': 4.350527967614511,\n",
       " 'sundaes are': 7.264430222920869,\n",
       " 'experience got': 5.221356325411908,\n",
       " 'loved brussels': 7.600902459542082,\n",
       " 'awesome removed': 9.210340371976184,\n",
       " 'oh move': 6.812445099177812,\n",
       " 'delicious flavors': 8.517193191416238,\n",
       " 'go almost': 9.210340371976184,\n",
       " 'cake moist': 6.16581793425276,\n",
       " 'sambal fish': 7.824046010856292,\n",
       " 'like cupcakes': 7.264430222920869,\n",
       " 'there lot': 4.290359446148058,\n",
       " 'near as': 7.600902459542082,\n",
       " \"they're getting\": 6.725433722188183,\n",
       " \"i'm out\": 6.812445099177812,\n",
       " 'everything...drinks wings': 8.517193191416238,\n",
       " 'street live': 8.517193191416238,\n",
       " 'random bite': 5.744604469176457,\n",
       " 'grinders': 7.1308988302963465,\n",
       " 'about bbq': 7.013115794639964,\n",
       " 'though already': 4.840892519509161,\n",
       " 'privileged have': 4.12893600699172,\n",
       " 'needed few': 6.571283042360924,\n",
       " 'confirm': 4.961845129926823,\n",
       " 'walked car': 4.779523573132869,\n",
       " 'service everything': 5.44914025628262,\n",
       " 'probably on': 6.119297918617867,\n",
       " 'what annoyed': 5.381698975487088,\n",
       " 'all caramel': 8.111728083308073,\n",
       " 'chicken spinach': 8.517193191416238,\n",
       " 'chowder so': 8.111728083308073,\n",
       " 'min limit': 5.149897361429764,\n",
       " 'community area': 8.517193191416238,\n",
       " 'massive': 4.226733750267846,\n",
       " 'taste nothing': 8.517193191416238,\n",
       " 'sat next': 4.721704002244043,\n",
       " 'happily recommend': 7.600902459542082,\n",
       " 'pickup order': 6.907755278982137,\n",
       " 'busier will': 7.600902459542082,\n",
       " 'borderline': 7.418580902748128,\n",
       " 'chicken crispy': 6.907755278982137,\n",
       " 'interactive place': 8.111728083308073,\n",
       " 'cheese which': 5.914503505971854,\n",
       " \"who's\": 4.12893600699172,\n",
       " 'back have': 5.8781358618009785,\n",
       " 'yet have': 8.517193191416238,\n",
       " 'adore': 7.600902459542082,\n",
       " 'jack': 6.214608098422191,\n",
       " 'fast also': 5.991464547107982,\n",
       " 'reminded me': 4.11659017116942,\n",
       " 'always delightful': 8.517193191416238,\n",
       " 'cheeseburger general': 4.06284589516273,\n",
       " \"then stan's\": 8.517193191416238,\n",
       " 'gross wax': 7.264430222920869,\n",
       " 'launches zone': 8.111728083308073,\n",
       " 'environment place': 7.418580902748128,\n",
       " \"along i'm\": 9.210340371976184,\n",
       " 'he should': 4.815891217303744,\n",
       " 'while certainly': 6.725433722188183,\n",
       " 'chicken liver': 8.111728083308073,\n",
       " 'main level': 8.111728083308073,\n",
       " 'seeing new': 7.600902459542082,\n",
       " 'caramelized': 3.6928874755114753,\n",
       " 'least': 2.7135653817903203,\n",
       " 'best feast': 8.517193191416238,\n",
       " 'www.yelp.com': 6.645391014514646,\n",
       " 'fluffy inches': 7.264430222920869,\n",
       " 'gems deserves': 6.502290170873972,\n",
       " 'place all': 6.571283042360924,\n",
       " \"you'll feel\": 7.264430222920869,\n",
       " 'saturday joe': 7.600902459542082,\n",
       " 'dente pasta': 9.210340371976184,\n",
       " 'beer which': 8.111728083308073,\n",
       " 'sapori before': 7.418580902748128,\n",
       " \"they're holding\": 8.517193191416238,\n",
       " 'being chi-town': 6.812445099177812,\n",
       " 'up stood': 5.952243833954701,\n",
       " 'to-eat': 7.824046010856292,\n",
       " 'sauce incredible': 9.210340371976184,\n",
       " 'showed up': 6.907755278982137,\n",
       " 'cheesy salty': 7.600902459542082,\n",
       " 'concerts': 7.824046010856292,\n",
       " 'really appreciated': 8.111728083308073,\n",
       " 'best so': 7.264430222920869,\n",
       " 'belmont': 6.032286541628237,\n",
       " \"since we'd\": 8.517193191416238,\n",
       " 'down spice': 8.111728083308073,\n",
       " 'used clean': 6.074846156047033,\n",
       " 'pasta cooked': 7.013115794639964,\n",
       " 'good ordered': 5.626821433520073,\n",
       " 'know drill': 6.645391014514646,\n",
       " 'pretty extensive': 6.437751649736401,\n",
       " \"cupcake can't\": 7.264430222920869,\n",
       " 'grill meticulously': 5.7763531674910364,\n",
       " 'bowl chicken': 9.210340371976184,\n",
       " 'strange drink': 6.3771270279199666,\n",
       " 'home fries': 7.824046010856292,\n",
       " 'usual will': 4.358310108056566,\n",
       " 'appreciated kind': 5.149897361429764,\n",
       " 'warned by': 5.381698975487088,\n",
       " \"be that's\": 8.111728083308073,\n",
       " 'coasts great': 9.210340371976184,\n",
       " 'packed flavor': 5.546778725846536,\n",
       " 'got pizza': 6.812445099177812,\n",
       " 'here yet': 7.1308988302963465,\n",
       " 'staying too': 5.149897361429764,\n",
       " 'explained us': 9.210340371976184,\n",
       " 'fierce chicago': 6.645391014514646,\n",
       " 'think water': 9.210340371976184,\n",
       " 'least minutes': 9.210340371976184,\n",
       " 'place highly': 8.517193191416238,\n",
       " \"it's behind\": 9.210340371976184,\n",
       " 'gluten-free vegetarian': 7.013115794639964,\n",
       " 'stars so': 9.210340371976184,\n",
       " 'week going': 6.645391014514646,\n",
       " 'fast there': 7.1308988302963465,\n",
       " 'one person': 5.683979847360021,\n",
       " 'run had': 7.1308988302963465,\n",
       " 'definitely give': 6.119297918617867,\n",
       " 'only half': 7.013115794639964,\n",
       " 'felt really': 6.645391014514646,\n",
       " 'fairly long': 9.210340371976184,\n",
       " 'been so': 3.989984546897858,\n",
       " 'tender liked': 6.725433722188183,\n",
       " 'bunch other': 9.210340371976184,\n",
       " 'seats by': 6.3771270279199666,\n",
       " 'say ours': 5.809142990314028,\n",
       " 'feel bad.\"': 4.8283137373023015,\n",
       " 'felt special': 7.013115794639964,\n",
       " 'buffalo sauce': 5.952243833954701,\n",
       " 'beer deep': 6.725433722188183,\n",
       " 'scramble': 5.339139361068292,\n",
       " 'up pizza': 6.502290170873972,\n",
       " 'consume hard': 5.240048458424061,\n",
       " 'gelato strawberries': 6.16581793425276,\n",
       " 'included oven': 6.812445099177812,\n",
       " 'unhappy guests': 5.381698975487088,\n",
       " 'fried garlic': 9.210340371976184,\n",
       " 'special cheese': 6.16581793425276,\n",
       " 'well packaged': 4.645992180508347,\n",
       " 'had added': 4.744432253321599,\n",
       " 'moist large': 6.571283042360924,\n",
       " 'free still': 7.1308988302963465,\n",
       " 'slow-cooked duck': 7.264430222920869,\n",
       " 'vibe restaurant': 5.521460917862246,\n",
       " 'amazing as': 7.600902459542082,\n",
       " 'job taking': 7.264430222920869,\n",
       " 'exception honestly': 7.1308988302963465,\n",
       " 'cut half': 6.907755278982137,\n",
       " 'live tell': 8.517193191416238,\n",
       " 'old friend': 6.571283042360924,\n",
       " 'glass windows': 8.517193191416238,\n",
       " 'worth me': 8.111728083308073,\n",
       " 'medium pizzas': 6.571283042360924,\n",
       " 'mediterranean bread': 4.892852258439873,\n",
       " 'perfect snack': 8.517193191416238,\n",
       " 'prohibition era': 7.1308988302963465,\n",
       " 'staff always': 6.725433722188183,\n",
       " 'service always': 4.29768548624013,\n",
       " 'lentil soup': 8.517193191416238,\n",
       " 'wings spice': 7.418580902748128,\n",
       " 'hiatus': 7.264430222920869,\n",
       " 'textured': 7.1308988302963465,\n",
       " 'definitely still': 8.517193191416238,\n",
       " 'between first': 9.210340371976184,\n",
       " 'return normal': 5.683979847360021,\n",
       " 'domes\" are': 5.298317366548036,\n",
       " 'one had': 5.005647752585217,\n",
       " 'tried needs': 8.517193191416238,\n",
       " 'are several': 7.264430222920869,\n",
       " 'though you': 7.264430222920869,\n",
       " \"i'd prob\": 5.843044541989709,\n",
       " 'dropped when': 5.149897361429764,\n",
       " 'really expected': 4.615220521841593,\n",
       " 'delicious would': 8.111728083308073,\n",
       " 'craving pizza': 6.502290170873972,\n",
       " 'there sometimes': 9.210340371976184,\n",
       " 'its watery': 5.403677882205863,\n",
       " 'comes are': 8.517193191416238,\n",
       " 'trying wings': 7.600902459542082,\n",
       " 'currently due': 4.961845129926823,\n",
       " 'time year': 8.517193191416238,\n",
       " 'look unique': 8.517193191416238,\n",
       " 'sour taste': 5.8781358618009785,\n",
       " 'sauce tone': 5.843044541989709,\n",
       " 'forgot name': 6.645391014514646,\n",
       " 'ah bad': 4.892852258439873,\n",
       " 'slow at': 5.914503505971854,\n",
       " 'over span': 4.721704002244043,\n",
       " \"i'll sure\": 7.418580902748128,\n",
       " 'washington dc': 5.546778725846536,\n",
       " 'dessert do': 8.111728083308073,\n",
       " 'pasta amazing': 8.111728083308073,\n",
       " \"certainly wouldn't\": 5.843044541989709,\n",
       " 'us cry': 7.600902459542082,\n",
       " \"should've put\": 8.517193191416238,\n",
       " 'should highlight': 7.418580902748128,\n",
       " 'our year': 5.339139361068292,\n",
       " 'old time': 9.210340371976184,\n",
       " 'mindset you': 7.824046010856292,\n",
       " 'sweet tooth': 7.1308988302963465,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'i',\n",
       " 'a',\n",
       " 'to',\n",
       " 'was',\n",
       " 'it',\n",
       " 'of',\n",
       " 'we',\n",
       " 'for',\n",
       " 'in',\n",
       " 'is',\n",
       " 'but',\n",
       " 'that',\n",
       " 'this',\n",
       " 'with',\n",
       " 'they',\n",
       " 'were',\n",
       " 'my',\n",
       " 'not']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
